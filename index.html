<!doctype html>
<html lang="en">
    <head>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <script src="./static/js/template.v2.js"></script>
        <script src="./static/js/contents_bar.js"></script>
        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>
        <script src="./static/js/cross_fade.js"></script>
        <link rel="stylesheet" href="style.css">
        
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
        

            
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>Detect-Describe-Discriminate</title>

            
            <style>
                p {
                font-size: 20px; /* Adjust the pixel value as needed */
                }
                .figcaption-font {
                font-size: 16px; /* Adjust the font size as needed */
                }

                ol {
                    margin-top: 0;
                    margin-bottom: 0;
                    list-style-position: inside; /* Adjust if necessary */

                }

                li {
                    line-height: 0.8;
                    padding-left: 100px; /* Adjust the padding value as needed */
                }
                /* Styling for the table */

                .pod-button-container {
                    display: flex;
                    justify-content: space-between;
                    margin-bottom: 10px; /* Vertical margin, adjust as needed */
                }
                .category-button {
                    padding: 10px 20px;
                    background-color: transparent; /* Default state */
                    border: 1px solid #ccc;
                    border-radius: 5px;
                    cursor: pointer;
                    font-size: 16px;
                    color: #000;
                    margin-right: 10px;
                    transition: background-color 0.3s, color 0.3s; /* Smooth transition */
                }

                .category-button:active, .category-button:focus {
                    background-color: rgb(149, 44, 15); /* Change background to blue when clicked or focused */
                    color: white; /* Change text to white */
                }
                .image-container {
                    border: 0px solid #ccc;
                    padding: 0px;
                    text-align: center;
                    margin-left:-20px;
                    margin-top:20px;
                    margin-bottom:20px;
                }
                #displayedImage {
                    max-width: auto;
                    height: 300px;
                }
                table, th, td {
                    border: 1px solid black;
                    border-collapse: collapse;
                }
                th, td {
                    padding: 10px;
                    text-align: left;
                    cursor: pointer;
                }
                .container {
                    display: flex;
                    align-items: start;
                    /* margin: 0 auto; */
                }

                .table-container {
                    margin-right: 10px;
                }

                /* Styling for the carousel */
                .carousel-container {
                    width: auto; 
                    margin-bottom: 2em;
                    position: relative;
                    overflow: hidden;
                    display: flex;
                    flex-direction: column; /* Stack elements vertically */
                    align-items: center;
                }

                .carousel-slide {
                    display: none;
                    width: auto;
                    height: auto;
                    position: relative;
                    display: flex;
                    justify-content: center;
                    align-items: center;
                    flex-direction: column; /* Stack image and caption vertically */
                }

                .carousel-slide.active {
                    display: flex;
                }

                .carousel-slide img {
                    width: 600px; /* Adjustable width */
                    height: auto; 
                    object-fit: contain;
                    margin: 0;
                }

                /* Caption Styling */
                .carousel-caption {
                    text-align: center;
                    margin-top: 10px;
                }

                .carousel-caption p {
                    font-size: 16px;
                    color: #333;
                    margin: 0;
                }

                .carousel-controls {
                    position: absolute;
                    top: 50%;
                    width: 100%;
                    display: flex;
                    justify-content: space-between;
                    transform: translateY(-50%);
                }

                .carousel-button {
                    background: none;
                    border: none;
                    cursor: pointer;
                    font-size: 24px;
                }
                
                .header-container {
                    background-color: transparent; /* Dark teal background */
                    color: black;
                    text-align: center; /* Center content on small screens */
                    padding: 20px; /* Reduced padding for small screens */
                }

                /* Additional styles can go here */
            </style>

    <!-- Zoom -->
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js"></script>  <!-- jquery -->
    <script defer src="./static/js/medium-zoom.min.js"></script>
    <script defer src="./static/js/zoom.js"></script>
    </head>
    <body>
        <div class="header-container">
            <div class="header-content">
              <h1><i>No Detail Left Behind</i>: Revisiting Self-Retrieval for Fine-Grained Image Captioning</h1>
              <div class="button-container">
                <a href="https://arxiv.org/abs/2409.03025" class="button">Paper</a>
                <a href="https://github.com/manugaurdl/EGG/tree/rll_refactor" class="button">Code</a>
              </div>
            </div>
            <div class="header-image">
                <img src="images/D3.png" alt="Teaser Image" class="teaser-image">
                <figcaption>
                    This image was generated using Flux.1 [schnell].
                </figcaption>
            </div>
        </div>
    <d-article>



        <div class="byline">
            <div class="byline-container">
                <p>
                    <a href="https://manugaurdl.github.io/" class="author-link" target="_blank" style="font-size: 1.3em; font-family: Charter;">Manu Gaur</a> &emsp;
                    <a href="https://darshansingh11.github.io/" class="author-link" target="_blank" style="font-size: 1.3em; font-family: Charter;">Darshan Singh S</a> &emsp;
                    <a href="https://makarandtapaswi.github.io/" class="author-link" target="_blank" style="font-size: 1.3em; font-family: Charter;">Makarand Tapaswi</a> &emsp;
                    <p></p>
                    <a href="https://www.iiit.ac.in/" class="affiliation-link" id="affiliation" target="_blank" style="font-size: 1.4em; font-family: Charter;">CVIT, IIIT Hyderabad</a>


                </p>
            </div>
        </div>
        
        <!-------------------CAROUSEL GANG!!!!-------------->
        
        <!-- <div class="carousel-container">
            <div class="carousel-slide active">
                <img src="images/vqa_5.jpg" alt="Teaser Image 1">
            </div>
            <div class="carousel-slide active">
                <img src="images/vqa_6.jpg" alt="Teaser Image 1">
            </div>
            <div class="carousel-slide active">
                <img src="images/vqa_2.jpg" alt="Teaser Image 2">
            </div>
            <div class="carousel-slide active">
                <img src="images/vqa_3.jpg" alt="Teaser Image 3">
            </div>
            <div class="carousel-slide active">
                <img src="images/vqa_4.jpg" alt="Teaser Image 4">
            </div>

            <div class="carousel-controls">
                <button class="carousel-button" onclick="moveSlide(-1)">&#10094;</button>
                <button class="carousel-button" onclick="moveSlide(1)">&#10095;</button>
            </div>
            <div class="carousel-caption">
                <p><i>Current MLLMs fail during D3 evaluation despite achieving perfect score in VQA, Swipe for more</i></p>
            </div>
        </div> -->

        
        
    
    <script>
    let slideIndex = 0;
    showSlides(slideIndex);


    var bibliography = {
        "1": "Manu Gaur, Darshan Singh S, Makarand Tapaswi. No Detail Left Behind: Revisiting Self-Retrieval for Fine-Grained Image Captioning. arXiv preprint arXiv:2409.03025, 2024",
        "2": "Xihui Liu, Hongsheng  Li, Jing  Shao, Dapeng Chen and Xiaogang Wang. Show, Tell and Discriminate: Image captioning by self-retrieval with partially labeled data. ECCV, 2024",
        "3": "Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs. arXiv preprint arXiv:2406.16860, 2024",
        "4": "Chameleon Team. Chameleon: Mixed-modal Early-Fusion Foundation Models. arXiv preprint arXiv:2405.09818, 2024.",
        "5": "Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-Next-Interleave: Tackling Multi-Image, Video, and 3D in Large Multimodal Models. arXiv preprint arXiv:2407.07895, 2024",
        "6": "Google. Gemini, 2023",
        "7": "OpenAI. GPT-4o, 2024",
        "8": "Anthropic. Claude-Sonnet, 2024",
        "9": "Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, YannLeCun, and Saining Xie. Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs. In Conference on Computer Vision and Pattern Recognition (CVPR), 2024",
        "10": "Penghao Wu and Saining Xie. V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs. In Conference on Computer Vision and Pattern Recognition (CVPR), 2024.",
    };

    // Function to insert citation
    function insertCitation(refId) {
        document.getElementById("ref" + refId).innerText = "[" + refId + "]";
    }

    // Function to update bibliography section
    function updateBibliography() {
        var bibliographySection = document.getElementById('bibliography');
        for (var key in bibliography) {
            if (bibliography.hasOwnProperty(key)) {
                var para = document.createElement("p");
                para.innerHTML = "[" + key + "] " + bibliography[key];
                bibliographySection.appendChild(para);
            }
        }
    }

    // Insert citations and update bibliography on page load
    document.addEventListener('DOMContentLoaded', (event) => {
        for (var i = 1; i <= 12; i++) {
            insertCitation(i.toString());
        }
        updateBibliography();
    });
    


    function cite(key) {
            var citationText = bibliography[key];
            if (citationText) {
                return "[" + key + "]";
            }
            return "[Citation not found]";
        }

    function moveSlide(n) {
        showSlides(slideIndex += n);
    }

    function showSlides(n) {
        let i;
        let slides = document.getElementsByClassName("carousel-slide");
        if (n >= slides.length) { slideIndex = 0 }
        if (n < 0) { slideIndex = slides.length - 1 }
        for (i = 0; i < slides.length; i++) {
            slides[i].style.display = "none";
        }
        slides[slideIndex].style.display = "block";
    }
    // This ensures the script runs after the HTML document is fully parsed
    document.addEventListener('DOMContentLoaded', (event) => {
            updateBibliography();
        });

</script>
        
        <!-- <d-contents>
            <nav>
                <h4>Contents</h4>
                <div><a href="#mmvp">Moving Beyond VQA</a></div>
                <div><a href="#patterns">MLLMs struggle with fine-grained Visual Discrimination</a></div>
                <div><a href="#patterns">Points of Difference in D<sub>3</sub> image pairs</a></div>
                <div><a href="#mof">Whitebox Evaluation of MLLMs</a></div>
            </nav>
        </d-contents> -->

        <!----------------------------------------------------INTRODUCTION -------------------------------------------------------------------->

        <d-figure id="teaser" >
            <figure>
                <img data-zoomable="" draggable="false" src="images/motorcycle_teaser.png" alt="motorcycles" style="width: 1000px; height: auto; display: block; margin: 0 auto;">
                <br>
                <figcaption class="figcaption-font" style="text-align: center; width: 1000px; margin: 0 auto;">
                    <strong>Figure 1</strong>: For a set of similar images, captioning systems struggle to uniquely describe each image. <strong>COCO MLE</strong>: A model trained on COCO with MLE generates the same description.
                    Our proposed method (<strong>OURS SR</strong>) generates discriminant captions while vanilla self-retrieval training(Dessì et al., 2023)(<strong>COCO SR</strong>) fails to, leading to hallucinations such as “two people” (middle).
                </figcaption>
            </figure>
        </d-figure>
        
        <p>
            Image captioning systems are unable to generate fine-grained captions, often using generic descriptions that fail to discriminate visually similar images. 
            In this work, <i>we holistically improve captioning systems by making their <strong>Training</strong> and <strong>Evaluation</strong> more fine-grained</i>:
            <br>
            <br>

            <div style="display: flex; flex-wrap: wrap; gap: 20px; align-items: start;">
                <div style="flex: 1; min-width: 300px;margin-top: 20px;">
                    <strong style="font-size: 22px">Towards Fine-grained Evaluation</strong>
                    <p style="font-size: 19px;">
                    <br>Traditional image captioning metrics fail to reward diversity or evaluate a model's fine-grained understanding ability.
                    Our first contribution addresses this by proposing <strong>self-retrieval</strong> (setup shown in <a href="#sr_setup">Figure 2</a>) from a lens of evaluating captioning systems: 
                    </p>
                    <br>
                    <ol>
                        <li style="line-height: 1.5; font-size: 19px;"><strong><a href="#truematch">Fine-grained Evaluation</a></strong>: We introduce <strong><i>TrueMatch</i></strong>, a benchmark comprising bags of highly similar images that uses self-retrieval to assess the captioner's ability to capture subtle visual distinctions.</li>
                    </ol>
                </div>
                
                <figure id ='sr_setup' style="flex: 1; min-width: 300px; margin: 0; margin-top:-40px;">
                    <img data-zoomable="" draggable="false" src="images/sr_setup.png" alt="motorcycles" style="width: 600px; height: auto; display: block;">
                    <figcaption class="figcaption-font" style="text-align: center; margin-top: 10px; margin-left:30px;margin-right: -80px; width:550px">
                        <strong>Figure 2:</strong> Self-retrieval judges the ability of a captioner to retrieve an image using its <i>generated caption</i> against of bag of distractor images. <a href="https://arxiv.org/pdf/2304.01662">Image source</a>.
                    </figcaption>
                </figure>
            </div>            



            <strong style="font-size: 22px"> Towards Fine-grained Training </strong><br>
            <p>Image captioning systems are unable to generate fine-grained captions as they are trained on data that is either noisy (alt-text) or generic (human annotations). This is further exacerbated by MLE training that encourages captioning systems to overuse common concepts and statistically probable phrases.
            We address this on two fronts: 
            <ol start="2">
                <li style="line-height: 1.5; font-size: 19px;"><strong><a href="#vcb">Data</a></strong>: We present <strong><i>Visual Caption Boosting</i></strong> to instill fine-grainedness in generic image captioning datasets while remaining anchored in human annotations.</li>
                <li style="line-height: 1.5; font-size: 19px;"><strong><a href="#training">Self-Retrieval Training</a></strong>: We design a <i>training recipe</i> to optimally leverage the self-retrieval reward and instill fine-grained visual information in the captioner.   
            </ol>
            <p style="font-size: 19px;">
            Jointly, they enable the captioner to describe fine-grained aspects in the image while preserving faithfulness to ground-truth captions.
            </p>
            </p>
        </p>

        <div class="icon-row">
            <a href="#truematch" class="icon-link">
                <img src="images/icons/eval.svg" alt="Eval Logo" class="icon">
                Evaluation
            </a>
            <a href="#vcb" class="icon-link">
                <img src="images/icons/visual.svg" alt="Visual Representation Logo" class="icon">
                Data
            </a>
            <a href="#training" class="icon-link">
                <img src="images/icons/recipe.svg" alt="Recipe Logo" class="icon">
                Training Recipe
            </a>
        </div>

        <p class="click-hint" style="width: 85%;">
            <img src="images/icons/click.gif" style="width: 1.5rem">
            <strong>Click to jump to each section.</strong>
        </p>

        <!------------------------------------------------Section 1: Evaluation--------------------------------------------------------->
        <section id="truematch">
            <h2>I. <i>TrueMatch</i>: Fine-grained Evaluation of Captioning Systems</h2>
            <p style="font-size: 19px;">

                <u>Reference-based metrics</u> such as BLEU, CIDEr and SPICE tend to penalize specificity, often favoring generic descriptions (Wang et al., 2020). <u>Reference-free metrics</u> like CLIPScore (Hessel et al., 2021) alleviate this issue by directly measuring image-text similarity, but may fail to encourage diverse and discriminant captions.

                <br><br>
                Existing self-retrieval (SR) approaches require models to select the target image from a set of N random distractor images(Dessì et al., 2023) which often have simple differences (e.g. the primary object or scene), making it easy for captioning systems to distinguish between them.
                In this work, we construct <strong>bags of highly similar images</strong>, where uniquely describing each image within a bag requires describing various facets of visual understanding. For example, retrieving the target image from <a href="#teaser"> Figure 1 </a> requires the caption to incorporate information about attributes <i>(red bike)</i> or orientation <i>(inverted body)</i>.             
                <span style="color: red;">we are doing SR evaluation</span>

                
                <br><br>
            </p>
            <div style="display: flex; flex-wrap: wrap; gap: 20px; align-items: start;">
                <div style="flex: 1; min-width: 300px;margin-top: 10px;">
                    <p style="font-size: 19px;">
                    <strong style="font-size: 25px">Benchmarking Captioning Systems on <i>TrueMatch</i></strong>
                    <br><br>
                    <a href="#table_1">Table 1</a> evaluates several open-source captioning approaches, MLLMs, and SR trained models on TrueMatch. 
                    <ul>
                        <li style="line-height: 1.2; font-size: 19px;"> Irrespective of their size, current captioners struggle to capture fine-grained visual details leading to poor performance on TrueMatch.</li>
                        <li style="line-height: 1.2; font-size: 19px;">  Our SR training recipe outperforms vanilla SR (DiscriTune) by +14.4% to +19.5%, demonstrating its effectiveness.</li>
                        <li style="line-height: 1.2; font-size: 19px;">  <strong>Cambrian-1</strong> is the best-performing open-source MLLM. Our proposed approach outperforms it significantly despite being 30x smaller.</li>
                    </ul>
                    </p>
                </div>
                
                <figure id ='table_1' style="flex: 1; min-width: 300px; margin: 0;margin-top:-10px">
                    <img data-zoomable="" draggable="false" src="images/table_1.png" alt="table1" style="width: 550px; height: auto; display: block;">
                    <figcaption class="figcaption-font" style="text-align: center; margin-top: 10px; margin-left:30px;margin-right: -80px; width:500px">
                        <strong>Table 1:</strong> Recall@1 for self-retrieval evaluation with TrueMatch is reported. The number
                        of bags in #3 is 254, #5 is 104, and #7 is 93
                    </figcaption>
                </figure>
            </div>  



        </section>
        
        <!------------------------------------------------Section 2: DATA--------------------------------------------------------------->

        <section id="vcb">
            <h2>II. <i>Visual Caption Boosting</i></h2>
        </section>
        <p style="font-size: 19px;">
            Existing image captioning datasets like COCO (Lin et al., 2014) comprise generic annotations such as <i>“There is a herd of zebras standing around” </i>(see Figure 3). 
            To address this, recent works leverage foundation models to synthetically expand visual information within
            captions. However they are prone to inherit biases present in foundation models (Sirotkin
            et al., 2022; Salman et al.).
            Additionally, these methods produce verbose descriptions that often exceed the token capacity of VLMs (CLIP with 77 tokens) and makes them prone to hallucinations (Favero et al., 2024).
            <br><br>

            We find that although the individual COCO captions are sparse, they describe complementary facets of the image, e.g. <i>“watering hole”, “bird flying over”, “herd of zebras”</i> (see Figure 3).

            VCB is a novel two stage framework leverages foundation models and the diverse perspectives offered by human annotators to generate rich descriptions while being anchored in human data:
        </p>
        
        <d-figure id="vcb_teaser" >
            <figure>
                <img data-zoomable="" draggable="false" src="images/vcb_teaser.png" alt="table1" style="width: 1100px; height: auto; display: block; margin: 0 auto;">
                <br>
                <figcaption class="figcaption-font" style="text-align: center; width: 1000px; margin: 0 auto;">
                    <strong>Figure 1:</strong> Analyzing the benchmarks.
                    <strong>Left:</strong> Performance comparison of MLLMs with visual input enabled and disabled across various benchmarks. 
                </figcaption>
            </figure>
        </d-figure>
        <p style="font-size: 19px;">
            <strong style="font-size: 22px">Stage 1: BlendCap</strong> leverages an off-the-shelf LLM to create a blended caption that combines multiple facets of visual information that the diverse human annotations capture. Notably, <i>we prompt the LLM to minimize redundant information resulting in short descriptions</i>.
                
            <br><br>

            <strong style="font-size: 22px">Stage 2: HolisticCap</strong> builds upon BlendCap by incorporating a fine-grained visual description produced by an MLLM. Specifically, we prompt the LLM to instill the visual caption into the blended caption while preferring human-grounded BlendCap in case of conflicting visual information. Additionally, the anchoring of semantic visual information in human annotations encourages the LLM to eliminate verbose tendencies of MLLMs, producing rich and succinct captions that capture fine-grained details.
            
            <span style="color: red;">split text b/w caption and body</span>
            <!-- this enables the LLM to ignore specific details from the visual caption such as “river or lake” and “drinking”, as they conflict with BlendCap’s description like “watering hole” and “grazing and standing”. -->

        
        <div style="display: flex; flex-wrap: wrap; gap: 20px; align-items: start;">
            <div style="flex: 1; min-width: 300px;margin-top: 30px;">
                <p style="font-size: 19px;">
                <strong style="font-size: 25px">Benchmarking Visually Boosted Captions on <i>TrueMatch</i></strong>
                <br>
                <ul>
                    <li style="line-height: 1.4; font-size: 19px;">BlendCap significantly outperforms original COCO captions on RD100 and  ClipScore, confirming that human annotations capture complementary visual aspects of the same image.
                    </li>
                    <li style="line-height: 1.4; font-size: 19px;">HolisticCap (unlike BlendCap) yields substantantial gains over COCO across bags of TrueMatch. 
                    </li>
                </ul>
                </p>
                <p style="font-size: 19px;">
                <span style="color: rgb(31, 104, 252);"><strong><i>This demonstrates the effectiveness of VCB in instilling fine-grained visual details into standard image captioning datasets.</i><strong></span></p>
            </div>              
            
            <figure id ='table_2' style="flex: 1; min-width: 300px; margin: 0;">
                <img data-zoomable="" draggable="false" src="images/table_2.png" alt="table1" style="width: 450px; height: auto; display: block;margin-left:0px;margin-top:50px">
                <figcaption class="figcaption-font" style="text-align: center; margin-top: 10px; margin-left:-20px;margin-right: -80px; width:500px">
                    <strong>Table 2:</strong> : R@1 scores for COCO and VCB captions evaluated on RD100 (100 random distractors) and TrueMatch.
                </figcaption>
            </figure>
        </div>  
        </p>

        <!-- moose -->
        <!------------------------------------------------Section 2: Training --------------------------------------------------------------->

        <section id="training">
            <h2>Guiding captioners away from their language modeling priors</h2>
           
            <p>
                Each image pair in D<sub>3</sub> are visually identical with one prominent <i>Point of Difference (POD)</i> distinguishing them. Similar to MMVP <span id="ref9"></span>, we identify 6 fine-grained <i>POD</i> that MLLMs struggle to identify: 

                <div class="container">
                    <div class="pod-button-container">
                        <button class="category-button" onclick="showImage('state')">State</button>
                        <button class="category-button" onclick="showImage('camera')">Camera</button>
                        <button class="category-button" onclick="showImage('orientation')">Orientation/Direction</button>
                        <button class="category-button" onclick="showImage('positioning')">Positioning</button>
                        <button class="category-button" onclick="showImage('scene')">Scene</button>
                        <button class="category-button" onclick="showImage('clutter')">Clutter</button>
                    </div>
                </div>
                
                <div class="container">
                    <div class="image-container">
                        <img id="displayedImage" src="images/POD/state.jpg" alt="Category Image">
                    </div>
                </div>
                <script>
                    var lastCategory = '';
                    function showImage(category) {
                        var imagePath = "images/POD/" + category + ".jpg";
                        var displayedImage = document.getElementById('displayedImage');
                        if (lastCategory === category) {
                            displayedImage.style.display = displayedImage.style.display === 'none' ? 'block' : 'none';
                        } else {
                            displayedImage.src = imagePath;
                            displayedImage.style.display = 'block';
                            lastCategory = category;
                        }
                    }
                </script>

                (Click on a specifc POD to see example)
               
            
            </p>
        </section>
        
        <section id="MoF">
            <h2>Whitebox Evaluation of MLLMs </h2>  
            <p>
                Since image pairs in D<sub>3</sub> benchmark have only one prominent POD, Self-Retrieval within D<sub>3</sub> enables whitebox evaluation. Specifically, when an MLLM fails to uniquely describe and retrieve both images within a pair, we are able to accurately identify which one of the following visual concept the MLLM is unable to pick up: 
        
                <d-figure>
                <img src="images/whitebox_eval.jpg" alt="SEAL-Bench Results" width="100%" style="display: block; margin-left: auto; margin-right: auto;">
                <figcaption>
                <strong>Whitebox Evaluation with D<sub>3</sub>:</strong> Self-retrieval performance of various MLLMs on image pairs of D<sub>3</sub> across all six points of difference.
                </figcaption>
                </d-figure>
            </p>
                We find that state-of-the-art MLLMs, both open and closed, struggle to perceive fine-grained changes in orientation/direction, camera angle, object's state, or positioning. This corroborates the findings of MMVP. In contrast, uniquely describing similar images with differing scenes appears to be easier for these models. V<sup>*</sup> <span id="ref10"></span> finds that MLLMs struggle to focus on fine-grained details in visually crowded images. While this is true for Cambrian-34B, closed-source MLLMs are capable of identifying characteristics of non-prominent objects to distinguish cluttered images.
            <p>

            </p>
        </section>
        
        </d-article>
        <d-appendix>
            <h3>BibTeX</h3>
            <p class="bibtex">
                @misc{gaur2024no,<br>
                &nbsp;&nbsp;title={Detect, Describe, Discriminate: Moving Beyond VQA for MLLM Evaluation},<br>
                &nbsp;&nbsp;author={Manu Gaur and Darshan Singh S and Makarand Tapaswi}<br>
                &nbsp;&nbsp;year={2024},<br>
                &nbsp;&nbsp;eprint={2409.15125},<br>
                &nbsp;&nbsp;archivePrefix={arXiv},<br>
                &nbsp;&nbsp;primaryClass={cs.CV}<br>
                }
            </p>

            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
          
        
        <div id="bibliography">
            <h3>Bibliography</h3>
            <ul>
                   
                </ul>
            </div>  
            
            <h3>Acknowledgements</h3>
            <p class="bibtex">
                This project page is built upon <a href="https://tsb0601.github.io/mmvp_blog/">MMVP</a>.
            </p>
        </d-appendix>

     
        <script>
          
            var dContents = document.querySelector('d-contents');
            var dArticle = document.querySelector('d-article');
            // Get the computed style of the element to access the margin
            var computedStyle = window.getComputedStyle(dContents);
            // Get the top margin as an integer
            var marginTop = parseInt(computedStyle.marginTop, 10);
            // Calculate the original top offset plus the margin-top
            var originalOffsetTop = dContents.offsetTop;
            var originalOffsetLeft = dContents.offsetLeft;
            var originalWidth = dContents.offsetWidth; // This should include padding if box-sizing is border-box
        
            // Function to handle the resize event
            function onResize() {
                // Recalculate original left and width on resize
                originalOffsetLeft = dContents.offsetLeft;
                originalWidth = dContents.offsetWidth; // This should include padding if box-sizing is border-box
            }
        
            // Add the resize event listener
            window.addEventListener('resize', onResize);
        
            window.addEventListener('scroll', function() {
                var scrollPosition = window.pageYOffset || document.documentElement.scrollTop;
                var dArticleBottom = dArticle.offsetTop + dArticle.offsetHeight;
                var dContentsActualTop = scrollPosition > originalOffsetTop ? scrollPosition : originalOffsetTop;
                var dContentsBottom = dContentsActualTop + dContents.offsetHeight;
                console.log("dArticleBottom", dArticleBottom)
                console.log("dContentsBottom", dContentsBottom)
                if (dContentsBottom >= dArticleBottom) {
                    // Make d-contents invisible
                    dContents.style.visibility = 'hidden';
                } else {
                    // Make d-contents visible
                    dContents.style.visibility = 'visible';
                }

                // Adjust the condition to account for margin-top
                if (scrollPosition + marginTop >= originalOffsetTop) {
                    dContents.style.position = 'fixed';
                    dContents.style.top = '0px';
                    dContents.style.left = originalOffsetLeft + 'px'; // Maintain the original horizontal position
                    dContents.style.width = originalWidth + 'px'; // Maintain the original width
                } else {
                    dContents.style.position = '';
                    dContents.style.top = '';
                    dContents.style.left = '';
                    dContents.style.width = ''; // Allow the width to be automatic
                }

                
            });

            
        
            // Initialize width and position
            onResize();
        </script>
        
        <script>
            // Function to determine which section is in view
            function getActiveSection() {
                var sections = document.querySelectorAll('section'); // Assuming your sections have a 'section' tag
                var scrollPosition = window.pageYOffset || document.documentElement.scrollTop;
        
                for (var i = 0; i < sections.length; i++) {
                    if (sections[i].offsetTop <= scrollPosition && sections[i].offsetTop + sections[i].offsetHeight > scrollPosition) {
                        return sections[i].id;
                    }
                }
                return null;
            }
        
            // Function to update the navigation items
            function updateNavigation() {
                var activeSection = getActiveSection();
                var navLinks = document.querySelectorAll('d-contents nav a');
        
                navLinks.forEach(function(navLink) {
                    if (navLink.getAttribute('href') === '#' + activeSection) {
                        navLink.classList.add('active-nav-item');
                    } else {
                        navLink.classList.remove('active-nav-item');
                    }
                });
            }
        
            // Add the scroll event listener
            window.addEventListener('scroll', updateNavigation);
        
            // Initial update
            updateNavigation();

                    // Insert citations and update bibliography on page load
            
        </script>
        
        
        

    </body>
</html>






<!--  -->

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Image Category Viewer</title>
    <style>
        body {
            font-family: Charter, serif;
            margin: 0;
            padding: 0px;
            box-sizing: border-box;
         }


    </style>
</head>
<body>
