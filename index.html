<!doctype html>
<html lang="en">
    <head>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <script src="./static/js/template.v2.js"></script>
        <script src="./static/js/contents_bar.js"></script>
        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>
        <script src="./static/js/cross_fade.js"></script>
        <link rel="stylesheet" href="style.css">
        
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
        

            
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>Detect-Describe-Discriminate</title>

            
            <style>
                d-article {
                overflow: visible; /* Ensure the article container allows overflow */
                }
                p {
                font-size: 20px; /* Adjust the pixel value as needed */
                }
                .figcaption-font {
                font-size: 16px; /* Adjust the font size as needed */
                }

                ol {
                    margin-top: 0;
                    margin-bottom: 0;
                    list-style-position: inside; /* Adjust if necessary */

                }

                li {
                    line-height: 0.8;
                    padding-left: 100px; /* Adjust the padding value as needed */
                }
                /* Styling for the table */

                .pod-button-container {
                    display: flex;
                    justify-content: space-between;
                    margin-bottom: 10px; /* Vertical margin, adjust as needed */
                }
                .category-button {
                    padding: 10px 20px;
                    background-color: transparent; /* Default state */
                    border: 1px solid #ccc;
                    border-radius: 5px;
                    cursor: pointer;
                    font-size: 16px;
                    color: #000;
                    margin-right: 10px;
                    transition: background-color 0.3s, color 0.3s; /* Smooth transition */
                }

                .category-button:active, .category-button:focus {
                    background-color: rgb(149, 44, 15); /* Change background to blue when clicked or focused */
                    color: white; /* Change text to white */
                }
                .image-container {
                    border: 0px solid #ccc;
                    padding: 0px;
                    text-align: center;
                    margin-left:-20px;
                    margin-top:20px;
                    margin-bottom:20px;
                }
                #displayedImage {
                    max-width: auto;
                    height: 300px;
                }
                table, th, td {
                    border: 1px solid black;
                    border-collapse: collapse;
                }
                th, td {
                    padding: 10px;
                    text-align: left;
                    cursor: pointer;
                }
                .container {
                    display: flex;
                    align-items: start;
                    /* margin: 0 auto; */
                }

                .table-container {
                    margin-right: 10px;
                }

                /* Styling for the carousel */
                .carousel-container {
                    width: auto; 
                    margin-bottom: 2em;
                    position: relative;
                    overflow: hidden;
                    display: flex;
                    flex-direction: column; /* Stack elements vertically */
                    align-items: center;
                }

                .carousel-slide {
                    display: none;
                    width: auto;
                    height: auto;
                    position: relative;
                    display: flex;
                    justify-content: center;
                    align-items: center;
                    flex-direction: column; /* Stack image and caption vertically */
                }

                .carousel-slide.active {
                    display: flex;
                }

                .carousel-slide img {
                    width: 600px; /* Adjustable width */
                    height: auto; 
                    object-fit: contain;
                    margin: 0;
                }

                /* Caption Styling */
                .carousel-caption {
                    text-align: center;
                    margin-top: 10px;
                }

                .carousel-caption p {
                    font-size: 16px;
                    color: #333;
                    margin: 0;
                }

                .carousel-controls {
                    position: absolute;
                    top: 50%;
                    width: 100%;
                    display: flex;
                    justify-content: space-between;
                    transform: translateY(-50%);
                }

                .carousel-button {
                    background: none;
                    border: none;
                    cursor: pointer;
                    font-size: 24px;
                }
                
                .header-container {
                    background-color: transparent; /* Dark teal background */
                    color: black;
                    text-align: center; /* Center content on small screens */
                    padding: 20px; /* Reduced padding for small screens */
                }

                /* Additional styles can go here */
            </style>

    <!-- Zoom -->
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js"></script>  <!-- jquery -->
    <script defer src="./static/js/medium-zoom.min.js"></script>
    <script defer src="./static/js/zoom.js"></script>
    </head>
    <body>
        <div class="header-container">
            <div class="header-content">
              <h1><i>No Detail Left Behind</i>: Revisiting Self-Retrieval for Fine-Grained Image Captioning</h1>
              <div class="button-container">
                <a href="https://arxiv.org/abs/2409.03025" class="button">Paper</a>
                <a href="https://github.com/manugaurdl/EGG/tree/rll_refactor" class="button">Code</a>
              </div>
            </div>
            <div class="header-image">
                <img src="images/D3.png" alt="Teaser Image" class="teaser-image">
                <figcaption>
                    This image was generated using Flux.1 [schnell].
                </figcaption>
            </div>
        </div>
    <d-article>



        <div class="byline">
            <div class="byline-container">
                <p>
                    <a href="https://manugaurdl.github.io/" class="author-link" target="_blank" style="font-size: 1.3em; font-family: Charter;">Manu Gaur</a> &emsp;
                    <a href="https://darshansingh11.github.io/" class="author-link" target="_blank" style="font-size: 1.3em; font-family: Charter;">Darshan Singh S</a> &emsp;
                    <a href="https://makarandtapaswi.github.io/" class="author-link" target="_blank" style="font-size: 1.3em; font-family: Charter;">Makarand Tapaswi</a> &emsp;
                    <p></p>
                    <a href="https://www.iiit.ac.in/" class="affiliation-link" id="affiliation" target="_blank" style="font-size: 1.4em; font-family: Charter;">CVIT, IIIT Hyderabad</a>


                </p>
            </div>
        </div>
        
        <!-------------------CAROUSEL GANG!!!!-------------->
        
        <!-- <div class="carousel-container">
            <div class="carousel-slide active">
                <img src="images/vqa_5.jpg" alt="Teaser Image 1">
            </div>
            <div class="carousel-slide active">
                <img src="images/vqa_6.jpg" alt="Teaser Image 1">
            </div>
            <div class="carousel-slide active">
                <img src="images/vqa_2.jpg" alt="Teaser Image 2">
            </div>
            <div class="carousel-slide active">
                <img src="images/vqa_3.jpg" alt="Teaser Image 3">
            </div>
            <div class="carousel-slide active">
                <img src="images/vqa_4.jpg" alt="Teaser Image 4">
            </div>

            <div class="carousel-controls">
                <button class="carousel-button" onclick="moveSlide(-1)">&#10094;</button>
                <button class="carousel-button" onclick="moveSlide(1)">&#10095;</button>
            </div>
            <div class="carousel-caption">
                <p><i>Current MLLMs fail during D3 evaluation despite achieving perfect score in VQA, Swipe for more</i></p>
            </div>
        </div> -->

        
        
    
    <script>
    let slideIndex = 0;
    showSlides(slideIndex);


    var bibliography = {
        "1": "Manu Gaur, Darshan Singh S, Makarand Tapaswi. No Detail Left Behind: Revisiting Self-Retrieval for Fine-Grained Image Captioning. arXiv preprint arXiv:2409.03025, 2024",
        "2": "Xihui Liu, Hongsheng  Li, Jing  Shao, Dapeng Chen and Xiaogang Wang. Show, Tell and Discriminate: Image captioning by self-retrieval with partially labeled data. ECCV, 2024",
        "3": "Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs. arXiv preprint arXiv:2406.16860, 2024",
        "4": "Chameleon Team. Chameleon: Mixed-modal Early-Fusion Foundation Models. arXiv preprint arXiv:2405.09818, 2024.",
        "5": "Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-Next-Interleave: Tackling Multi-Image, Video, and 3D in Large Multimodal Models. arXiv preprint arXiv:2407.07895, 2024",
        "6": "Google. Gemini, 2023",
        "7": "OpenAI. GPT-4o, 2024",
        "8": "Anthropic. Claude-Sonnet, 2024",
        "9": "Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, YannLeCun, and Saining Xie. Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs. In Conference on Computer Vision and Pattern Recognition (CVPR), 2024",
        "10": "Penghao Wu and Saining Xie. V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs. In Conference on Computer Vision and Pattern Recognition (CVPR), 2024.",
    };

    // Function to insert citation
    function insertCitation(refId) {
        document.getElementById("ref" + refId).innerText = "[" + refId + "]";
    }

    // Function to update bibliography section
    function updateBibliography() {
        var bibliographySection = document.getElementById('bibliography');
        for (var key in bibliography) {
            if (bibliography.hasOwnProperty(key)) {
                var para = document.createElement("p");
                para.innerHTML = "[" + key + "] " + bibliography[key];
                bibliographySection.appendChild(para);
            }
        }
    }

    // Insert citations and update bibliography on page load
    document.addEventListener('DOMContentLoaded', (event) => {
        for (var i = 1; i <= 12; i++) {
            insertCitation(i.toString());
        }
        updateBibliography();
    });
    


    function cite(key) {
            var citationText = bibliography[key];
            if (citationText) {
                return "[" + key + "]";
            }
            return "[Citation not found]";
        }

    function moveSlide(n) {
        showSlides(slideIndex += n);
    }

    function showSlides(n) {
        let i;
        let slides = document.getElementsByClassName("carousel-slide");
        if (n >= slides.length) { slideIndex = 0 }
        if (n < 0) { slideIndex = slides.length - 1 }
        for (i = 0; i < slides.length; i++) {
            slides[i].style.display = "none";
        }
        slides[slideIndex].style.display = "block";
    }
    // This ensures the script runs after the HTML document is fully parsed
    document.addEventListener('DOMContentLoaded', (event) => {
            updateBibliography();
        });

</script>
        
        <!-- <d-contents>
            <nav>
                <h4>Contents</h4>
                <div><a href="#mmvp">Moving Beyond VQA</a></div>
                <div><a href="#patterns">MLLMs struggle with fine-grained Visual Discrimination</a></div>
                <div><a href="#patterns">Points of Difference in D<sub>3</sub> image pairs</a></div>
                <div><a href="#mof">Whitebox Evaluation of MLLMs</a></div>
            </nav>
        </d-contents> -->

        <!----------------------------------------------------INTRODUCTION -------------------------------------------------------------------->

        <d-figure id="teaser" >
            <figure>
                <img data-zoomable="" draggable="false" src="images/motorcycle_teaser.png" alt="motorcycles" style="width: 1000px; height: auto; display: block; margin: 0 auto;">
                <br>
                <figcaption class="figcaption-font" style="text-align: center; width: 1000px; margin: 0 auto;">
                    <strong>Figure 1</strong>: For a set of similar images, captioning systems struggle to uniquely describe each image. <strong>COCO MLE</strong>: A model trained on COCO with MLE generates the same description.
                    Our proposed method (<strong>OURS SR</strong>) generates discriminant captions while vanilla self-retrieval training(Dessì et al., 2023)(<strong>COCO SR</strong>) fails to, leading to hallucinations such as “two people” (middle).
                </figcaption>
            </figure>
        </d-figure>
        
        <p>
            Image captioning systems are unable to generate fine-grained captions, often using generic descriptions that fail to discriminate visually similar images. 
            In this work, <i>we holistically improve captioning systems by making their <strong>Training</strong> and <strong>Evaluation</strong> more fine-grained</i>:
            <br>
            <br>

            <div style="display: flex; flex-wrap: wrap; gap: 20px; align-items: start;">
                <div style="flex: 1; min-width: 300px;margin-top: 20px;">
                    <strong style="font-size: 22px">Towards Fine-grained Evaluation</strong>
                    <p style="font-size: 19px;">
                    <br>Traditional image captioning metrics fail to reward diversity or evaluate a model's fine-grained understanding ability.
                    Our first contribution addresses this by proposing <strong>self-retrieval</strong> (setup shown in <a href="#sr_setup">Figure 2</a>) from a lens of evaluating captioning systems: 
                    </p>
                    <br>
                    <ol>
                        <li style="line-height: 1.5; font-size: 19px;"><strong><a href="#truematch">Fine-grained Evaluation</a></strong>: We introduce <strong><i>TrueMatch</i></strong>, a benchmark comprising bags of highly similar images that uses self-retrieval to assess the captioner's ability to capture subtle visual distinctions.</li>
                    </ol>
                </div>
                
                <figure id ='sr_setup' style="flex: 1; min-width: 300px; margin: 0; margin-top:-40px;">
                    <img data-zoomable="" draggable="false" src="images/sr_setup.png" alt="motorcycles" style="width: 600px; height: auto; display: block;">
                    <figcaption class="figcaption-font" style="text-align: center; margin-top: 10px; margin-left:30px;margin-right: -80px; width:550px">
                        <strong>Figure 2:</strong> Self-retrieval judges the ability of a captioner to retrieve an image using its <i>generated caption</i> against of bag of distractor images. <a href="https://arxiv.org/pdf/2304.01662">Image source</a>.
                    </figcaption>
                </figure>
            </div>            



            <strong style="font-size: 22px"> Towards Fine-grained Training </strong><br>
            <p>Image captioning systems are unable to generate fine-grained captions as they are trained on data that is either noisy (alt-text) or generic (human annotations). This is further exacerbated by MLE training that encourages captioning systems to overuse common concepts and statistically probable phrases.
            We address this on two fronts: 
            <ol start="2">
                <li style="line-height: 1.5; font-size: 19px;"><strong><a href="#vcb">Data</a></strong>: We present <strong><i>Visual Caption Boosting</i></strong> to instill fine-grainedness in generic image captioning datasets while remaining anchored in human annotations.</li>
                <li style="line-height: 1.5; font-size: 19px;"><strong><a href="#training">Self-Retrieval Training</a></strong>: We design a <i>training recipe</i> to optimally leverage the self-retrieval reward and instill fine-grained visual information in the captioner.   
            </ol>
            <p style="font-size: 19px;">
            Jointly, they enable the captioner to describe fine-grained aspects in the image while preserving faithfulness to ground-truth captions.
            </p>
            </p>
        </p>

        <div class="icon-row">
            <a href="#truematch" class="icon-link">
                <img src="images/icons/eval.svg" alt="Eval Logo" class="icon">
                Evaluation
            </a>
            <a href="#vcb" class="icon-link">
                <img src="images/icons/visual.svg" alt="Visual Representation Logo" class="icon">
                Data
            </a>
            <a href="#training" class="icon-link">
                <img src="images/icons/recipe.svg" alt="Recipe Logo" class="icon">
                Training Recipe
            </a>
        </div>

        <p class="click-hint" style="width: 85%;">
            <img src="images/icons/click.gif" style="width: 1.5rem">
            <strong>Click to jump to each section.</strong>
        </p>

        <!------------------------------------------------Section 1: Evaluation--------------------------------------------------------->
        <section id="truematch">
            <h2>I. <i>TrueMatch</i>: Fine-grained Evaluation of Captioning Systems</h2>
            <p style="font-size: 19px;">

                <u>Reference-based metrics</u> such as BLEU, CIDEr and SPICE tend to penalize specificity, often favoring generic descriptions (Wang et al., 2020). <u>Reference-free metrics</u> like CLIPScore (Hessel et al., 2021) alleviate this issue by directly measuring image-text similarity, but may fail to encourage diverse and discriminant captions.

                <br><br>
                Existing self-retrieval (SR) approaches require models to select the target image from a set of N random distractor images (Dessì et al., 2023) which often have simple differences (e.g. the primary object or scene), making it easy for captioning systems to distinguish between them.
                In this work, we construct <strong>bags of highly similar images</strong>, where uniquely describing each image within a bag requires describing various facets of visual understanding. For example, retrieving the target image from <a href="#teaser"> Figure 1 </a> requires the caption to incorporate information about attributes <i>(red bike)</i> or orientation <i>(inverted body)</i>.             
                <span style="color: red;">we are doing SR evaluation</span>

                
                <br><br>
            </p>
            <div style="display: flex; flex-wrap: wrap; gap: 20px; align-items: start;">
                <div style="flex: 1; min-width: 300px;margin-top: 10px;">
                    <p style="font-size: 19px;">
                    <strong style="font-size: 25px">Benchmarking Captioning Systems on <i>TrueMatch</i></strong>
                    <br><br>
                    <a href="#table_1">Table 1</a> evaluates several open-source captioning approaches, MLLMs, and SR trained models on TrueMatch. 
                    <ul>
                        <li style="line-height: 1.2; font-size: 19px;"> Irrespective of their size, current captioners struggle to capture fine-grained visual details leading to poor performance on TrueMatch.</li>
                        <li style="line-height: 1.2; font-size: 19px;">  Our SR training recipe outperforms vanilla SR (DiscriTune) by +14.4% to +19.5%, demonstrating its effectiveness.</li>
                        <li style="line-height: 1.2; font-size: 19px;">  <strong>Cambrian-1</strong> is the best-performing open-source MLLM. Our proposed approach outperforms it significantly despite being 30x smaller.</li>
                    </ul>
                    </p>
                </div>
                
                <figure id ='table_1' style="flex: 1; min-width: 300px; margin: 0;margin-top:-10px">
                    <img data-zoomable="" draggable="false" src="images/table_1.png" alt="table1" style="width: 550px; height: auto; display: block;">
                    <figcaption class="figcaption-font" style="text-align: center; margin-top: 10px; margin-left:30px;margin-right: -80px; width:500px">
                        <strong>Table 1:</strong> Recall@1 for self-retrieval evaluation with TrueMatch is reported. The number
                        of bags in #3 is 254, #5 is 104, and #7 is 93
                    </figcaption>
                </figure>
            </div>  



        </section>
        
        <!------------------------------------------------Section 2: DATA--------------------------------------------------------------->

        <section id="vcb">
            <h2>II. <i>Visual Caption Boosting</i></h2>
        </section>
        <p style="font-size: 19px;">
            Existing image captioning datasets like COCO (Lin et al., 2014) comprise generic annotations such as <i>“There is a herd of zebras standing around” </i>(see Figure 3). 
            To address this, recent works leverage foundation models to synthetically expand visual information within
            captions. However they are prone to inherit biases present in foundation models (Sirotkin
            et al., 2022; Salman et al.).
            Additionally, these methods produce verbose descriptions that often exceed the token capacity of VLMs (CLIP with 77 tokens) and makes them prone to hallucinations (Favero et al., 2024).
            <br><br>

            We find that although the individual COCO captions are sparse, they describe complementary facets of the image, e.g. <i>“watering hole”, “bird flying over”, “herd of zebras”</i> (see Figure 3).

            VCB is a novel two stage framework leverages foundation models and the diverse perspectives offered by human annotators to generate rich descriptions while being anchored in human data:
        </p>
        
        <d-figure id="vcb_teaser" >
            <figure>
                <img data-zoomable="" draggable="false" src="images/vcb_teaser.png" alt="table1" style="width: 1100px; height: auto; display: block; margin: 0 auto;">
                <br>
                <figcaption class="figcaption-font" style="text-align: center; width: 1000px; margin: 0 auto;">
                    <strong>Figure 1:</strong> Analyzing the benchmarks.
                    <strong>Left:</strong> Performance comparison of MLLMs with visual input enabled and disabled across various benchmarks. 
                </figcaption>
            </figure>
        </d-figure>
        <p style="font-size: 19px;">
            <strong style="font-size: 22px">Stage 1: BlendCap</strong> leverages an off-the-shelf LLM to create a blended caption that combines multiple facets of visual information that the diverse human annotations capture. Notably, <i>we prompt the LLM to minimize redundant information resulting in short descriptions</i>.
                
            <br><br>

            <strong style="font-size: 22px">Stage 2: HolisticCap</strong> builds upon BlendCap by incorporating a fine-grained visual description produced by an MLLM. Specifically, we prompt the LLM to instill the visual caption into the blended caption while preferring human-grounded BlendCap in case of conflicting visual information. Additionally, the anchoring of semantic visual information in human annotations encourages the LLM to eliminate verbose tendencies of MLLMs, producing rich and succinct captions that capture fine-grained details.
            
            <span style="color: red;">split text b/w caption and body</span>
            <!-- this enables the LLM to ignore specific details from the visual caption such as “river or lake” and “drinking”, as they conflict with BlendCap’s description like “watering hole” and “grazing and standing”. -->
        </p>
        
        <div style="display: flex; flex-wrap: wrap; gap: 20px; align-items: start;">
            <div style="flex: 1; min-width: 300px;margin-top: 30px;">
                <p style="font-size: 19px;">
                <strong style="font-size: 25px">Benchmarking Visually Boosted Captions on <i>TrueMatch</i></strong>
                <br>
                <ul>
                    <li style="line-height: 1.4; font-size: 19px;">BlendCap significantly outperforms original COCO captions on RD100 and  ClipScore, confirming that human annotations capture complementary visual aspects of the same image.
                    </li>
                    <li style="line-height: 1.4; font-size: 19px;">HolisticCap (unlike BlendCap) yields substantantial gains over COCO across bags of TrueMatch. 
                    </li>
                </ul>
                </p>
                
            </div>              
            
            <figure id ='table_2' style="flex: 1; min-width: 300px; margin: 0;">
                <img data-zoomable="" draggable="false" src="images/table_2.png" alt="table1" style="width: 450px; height: auto; display: block;margin-left:10px;margin-top:20px">
                <figcaption class="figcaption-font" style="text-align: center; margin-top: 10px; margin-left:-20px;margin-right: -80px; width:500px">
                    <strong>Table 2:</strong> : R@1 scores for COCO and VCB captions evaluated on RD100 (100 random distractors) and TrueMatch.
                </figcaption>
            </figure>
        </div>
        <br>
        <p style="font-size: 19px;">
            <span style="color: rgb(31, 104, 252);"><strong><i>This demonstrates the effectiveness of VCB in instilling fine-grained visual details into standard image captioning datasets.</i><strong></span></p>


        <!------------------------------------------------Section 3: Training --------------------------------------------------------------->

        <section id="training">
            <h2>III. Guiding captioners away from their language modeling priors</h2>

            <!-- CLIPCAP -->
            <div style="display: flex; flex-wrap: wrap; gap: 20px; width: 1500px; align-items: start;">
                <div style="flex: 1; margin-top: 30px;">
                    <p style="font-weight: normal; font-size: 19px;">
            
                        We adopt ClipCap (Mokady et al., 2021), a lightweight (200M) simplification of the modern MLLMs.
                        Training the captioning system has two stages: 
                        <ol>
                            <li style="line-height: 1; font-size: 19px;font-weight: normal;">Maximum Likelihood Estimation (MLE) pretraining.</li>
                            <li style="line-height: 1.2; font-size: 19px;font-weight: normal;">REINFORCE fine-tuning by maximizing the self-retrieval (SR) reward.</li>
                        </ol>
        
                    </p>
                </div>              
                
                <figure id ='table_2' style="flex: 1; min-width: 300px; margin: 0;">
                    <img data-zoomable="" draggable="false" src="images/clipcap.jpg" alt="table1" style="width: 350px; height: auto; display: block;margin-left:0px;margin-top:0px">
                    <figcaption class="figcaption-font" style="text-align: center; margin-top: 0px; margin-left:-20px; width:380px">
                        <strong>Figure 4:</strong> : ClipCap connects a CLIP visual encoder to a GPT-2 through a simple MLP adapter.
                    </figcaption>
                </figure>
            </div>  
        
        <br>
        <p style="font-weight: normal; font-size: 19px;">
        <!------------------------------------------------SR-L --------------------------------------------------------------->
        <strong style="font-size: 25px">Fine-tuning the Language Model with Self-Retrieval (SR-L): A Deep Dive</strong>
        <br><br>
        <strong>MLE encourages generation of generic descriptions.</strong> Retrieval performance of MLE trained captioner reduces substantially for all datasets (rows 1-3) compared to their ground-truth captions (see Table 2).

        <br><br>
        
        <strong> Self-retrieval fine-tuning benefits from a rich MLE initialization.</strong>
        SR-L fine-tuning with <span style="background-color: rgb(0, 255, 0); color: black;">HolisticCap</span> significantly outperforms <span style="background-color: lightgray; color: black;">COCO</span> (Dessì et al., 2023) on TrueMatch.
        <!-- SR-L overcomes the limitations of MLE by making the captions more discriminant, leading to consistent improvements in RD100 and TrueMatch across all 3 captions (compare rows 1-4, 2-5, 3-6).  -->
        </p>
        <d-figure id="table3" >
            <figure>
                <img data-zoomable="" draggable="false" src="images/table_3.png" alt="table1" style="width:660px; height: auto; display: block; margin: 0 auto;">
                <br>
                <figcaption class="figcaption-font" style="text-align: center; width: 660px; margin: 0 auto;">
                    <strong>Table 3:</strong> Results across different training approaches (MLE, SR-L) and captions (COCO, BlendCap, HolisticCap) show the effectiveness of improved initialization with HolisticCap and SR-L fine-tuning.
                </figcaption>
            </figure>
        </d-figure>
        <p style="font-weight: normal; font-size: 19px;">
        <strong> Self-retrieval unlocks latent semantic information when fine-tuning the LLM. </strong>
        
        The model trained on COCO, due to the MLE objective, generates sparse captions that resemble the independent annotations of COCO. 
        This leads to a large gap between <span style="background-color: lightgray; color: black;">COCO</span> and <span style="background-color:lightgreen; color: black;">BlendCap</span> on RD100 (rows 1, 2), despite both having similar semantic information.
        Interestingly, SR-L fine-tuning narrows this gap dramatically (rows 4, 5) while preserving caption faithfulness.
        </p>
        <p style="font-size: 19px;">
            <span style="color: rgb(31, 104, 252);"><strong><i>This is remarkable, as it demonstrates that self-retrieval fine-tuning steers the language model to unlock latent semantic information within its embedding space, even when it was initially obscured by MLE.</i><strong></span></p>
        
        
        <!------------------------------------------------TRADEOFF --------------------------------------------------------------->
        <br>
        <p style="font-size: 25px;"> Retrieval Performance vs Caption Faithfulness : The Trade-off plaguing SR fine-tuning</p>

        <div style="display: flex; flex-wrap: wrap; gap: 20px; align-items: start;">
            <div style="flex: 1; min-width: 300px;margin-top: 20px; margin-left:20px ;">
                <p style="font-size:19px;font-weight: normal;">
                
                    <ul>
                        <li style="line-height: 1.2; font-size: 19px; font-weight: normal; margin-bottom: 30px;"> We observe that captioning system's fine-tuned with vanilla SR-L (Dessì et al. (2023)) become less faithful to the ground-truth captions upon extended training.</li>
                        <li style="line-height: 1.2; font-size: 19px; font-weight: normal; margin-bottom: 30px">As seen in Figure 5, retrieval performance continually improves while CIDEr dips significantly.</li>
                        <li style="line-height: 1.2; font-size: 19px; font-weight: normal;"> We also find that captioners fine-tuned with SR, in a dash to enhance retrieval performance, have a tendency to <strong>hallucinate details</strong>.
                        </li>
                    </ul>
                </p>        
            </div>              
            
            <figure id ='table_2' style="flex: 1; min-width: 300px; margin: 0;">
                <img data-zoomable="" draggable="false" src="images/tradeoff.png" alt="table1" style="width: 450px; height: auto; display: block;margin-left:60px;margin-top:10px">
                <figcaption class="figcaption-font" style="text-align: center; margin-top:0px; margin-left:80px; width:450px;font-weight: normal;">
                    <strong>Table 2:</strong> : R@1 scores for COCO and VCB captions evaluated on RD100 (100 random distractors) and TrueMatch.
                </figcaption>
            </figure>
        </div>  
        <!---------------------------------------- SR-V ------------------------------------------>
        <p style="font-size: 20px;"> Fine-tuning the Visual Encoder with Self-Retrieval (SR-V)</p>
        <p style="font-size:19px;font-weight: normal;">
        
        While fine-tuning CLIP with SR yields superior retrieval performance, it makes the captioner less faithful to the ground-truth captions.
        We also find that SR-V fine-tuning worsens <i>SR's tendency to hallucinate</i> and <i>attribute binding</i> in captioners. 
        </p>

        
        <!------------------------------------------------BagCurri --------------------------------------------------------------->
        <br><br>
        <p style="font-size: 25px;"> BagCurri: Designing a Self-Retrieval Learning Curriculum with Bags of Hard Negatives</p>
        <p style="font-size:19px;font-weight: normal;">
        
        For a stronger learning signal, we fine-tune with bags of highly similar images within a minibatch instead of retrieving against 99 random distractors (vanilla SR).
        We also propose a curriculum over bag sizes (see Figure 6) to <i>more optimally leverage the contrastive SR reward</i>.</p>
        
        <d-figure id="bagcurri" style="overflow-x: auto; width: 100%;" >
            <figure>
                <img data-zoomable="" draggable="false" src="images/bagcurri_tab_fig.png" alt="table1" style="width: 1200px; height: auto; display: block;margin-left:-20px;">
                <br>
                <figcaption class="figcaption-font" style="text-align: center; width: 1000px; margin: 0 auto;">
                    <strong>Figure 1:</strong> Analyzing the benchmarks.
                </figcaption>
            </figure>
        </d-figure>

        <strong style="font-size: 20px;"">SR-V fine-tuning with BagCurri</strong>
        <ul style="padding-left: 30px;">
            <li style="line-height: 1.2; font-size: 19px; font-weight: normal; margin-bottom: 5px;">Forces CLIP to learn fine-grained visual features leading to <i>superior retrieval performance</i> on TrueMatch. </li>
            <li style="line-height: 1.2; font-size: 19px; font-weight: normal; margin-bottom: 0px">Reduces caption faithfulness, leading to increased hallucinations and worse attribute binding.</li>
        </ul>

        <strong style="font-size: 20px;"">SR-L fine-tuning with BagCurri</strong>.
        <ul style="padding-left: 30px;">
            <li style="line-height: 1.2; font-size: 19px; font-weight: normal; margin-bottom: 5px;">BagCurri is too challenging for SR-L fine-tuning, unable to yield meaningful retrieval gains over vanilla SR-L. </li>
            <li style="line-height: 1.2; font-size: 19px; font-weight: normal; margin-bottom: 0px">Benefits from rich MLE initialization provided by HolisticCap: <i>CIDEr score increases</i> compared to vanilla SR-L.</li>
        </ul>
        <br>
        <p style="font-size:19px; font-weight: normal;">
        <strong style="font-size: 20px;"">BEST OF BOTH WORLDS: SR-LV fine-tuning with BagCurri</strong>.
        <br>We initialize the captioner with HolisticCap and fine-tune both the language and visual components (SR-LV) with BagCurri.
        This results in the most visually fine-grained model while also improving CIDEr over the MLE trained captioner.
        Notably, we find that <i>BagCurri is solely responsible for preserving caption faithfulness</i>, with CIDEr decreasing when bags are used without our curriculum.
        </p>
        <p style="font-size: 19px;">
        <span style="color: rgb(31, 104, 252);"><strong><i>This is momentous, as without modifying the model's architecture or reward function, we significantly outperform vanilla self-retrieval on TrueMatch (average 15%), while circumventing the caption faithfulness trade-off plaguing current SR approaches. </i><strong></span></p>
        </p>
        <br>
        <!------------------------------------------------CIDEr + SR --------------------------------------------------------------->
        <p style="font-size: 25px;"> CIDEr Optimization meets SR Fine-Tuning</p>

        <div style="display: flex; flex-wrap: wrap; gap: 20px; align-items: start;">
            <div style="flex: 1; min-width: 300px;margin-top: 20px; margin-left:20px ;">
                <p style="font-size:19px;font-weight: normal;">
                
                    <ul>
                        <li style="line-height: 1.2; font-size: 19px; font-weight: normal; margin-bottom: 30px;">CIDEr opt. (<span style="background-color: lightgreen; color: black;">row 8</span>) outperforms the MLE trained model (<span style="background-color:lightgray; color: black;">row 6</span>) on TrueMatch, only when initialized with HolisticCap.</li>
                        <li style="line-height: 1.2; font-size: 19px; font-weight: normal;">Even CIDEr opt. (<span style="background-color: rgb(255, 102, 102); color: black;">row 5</span>) is unable to preseve caption faithfulness for COCO, underscoring the <u>importance of initialization</u> during SR fine-tuning. </li>
                        <li style="line-height: 1.2; font-size: 19px; font-weight: normal; margin-bottom: 30px">Joint optimization with HolisticCap (<span style="background-color: rgb(0, 255, 0); color: black;">row 10</span>) results in the most discriminant model while <i>significantly improving CIDEr </i> over MLE pretraining (<span style="background-color:lightgray; color: black;">row 6</span>).</li>
                        </li>
                    </ul>
                </p>        
            </div>           
            
            <figure id ='table_2' style="flex: 1; min-width: 300px; margin: 0;">
                <img data-zoomable="" draggable="false" src="images/table_5.png" alt="table1" style="width: 550px; height: auto; display: block;margin-left:60px;margin-top:-20px">
                <figcaption class="figcaption-font" style="text-align: center; margin-top:0px; margin-left:80px; width:450px;font-weight: normal;">
                    <strong>Table 2:</strong> : R@1 scores for COCO and VCB captions evaluated on RD100 (100 random distractors) and TrueMatch.
                </figcaption>
            </figure>
        </div>  
        

        
        
         
         
        
        <!-- moose -->
        <!------------------------------------------------END --------------------------------------------------------------->

        </d-article>
        <d-appendix>
            <h3>BibTeX</h3>
            <p class="bibtex">
                @misc{gaur2024no,<br>
                &nbsp;&nbsp;title={Detect, Describe, Discriminate: Moving Beyond VQA for MLLM Evaluation},<br>
                &nbsp;&nbsp;author={Manu Gaur and Darshan Singh S and Makarand Tapaswi}<br>
                &nbsp;&nbsp;year={2024},<br>
                &nbsp;&nbsp;eprint={2409.15125},<br>
                &nbsp;&nbsp;archivePrefix={arXiv},<br>
                &nbsp;&nbsp;primaryClass={cs.CV}<br>
                }
            </p>

            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
          
        
        <div id="bibliography">
            <h3>Bibliography</h3>
            <ul>
                   
                </ul>
            </div>  
            
            <h3>Acknowledgements</h3>
            <p class="bibtex">
                This project page is built upon <a href="https://tsb0601.github.io/mmvp_blog/">MMVP</a>.
            </p>
        </d-appendix>

     
        <script>
          
            var dContents = document.querySelector('d-contents');
            var dArticle = document.querySelector('d-article');
            // Get the computed style of the element to access the margin
            var computedStyle = window.getComputedStyle(dContents);
            // Get the top margin as an integer
            var marginTop = parseInt(computedStyle.marginTop, 10);
            // Calculate the original top offset plus the margin-top
            var originalOffsetTop = dContents.offsetTop;
            var originalOffsetLeft = dContents.offsetLeft;
            var originalWidth = dContents.offsetWidth; // This should include padding if box-sizing is border-box
        
            // Function to handle the resize event
            function onResize() {
                // Recalculate original left and width on resize
                originalOffsetLeft = dContents.offsetLeft;
                originalWidth = dContents.offsetWidth; // This should include padding if box-sizing is border-box
            }
        
            // Add the resize event listener
            window.addEventListener('resize', onResize);
        
            window.addEventListener('scroll', function() {
                var scrollPosition = window.pageYOffset || document.documentElement.scrollTop;
                var dArticleBottom = dArticle.offsetTop + dArticle.offsetHeight;
                var dContentsActualTop = scrollPosition > originalOffsetTop ? scrollPosition : originalOffsetTop;
                var dContentsBottom = dContentsActualTop + dContents.offsetHeight;
                console.log("dArticleBottom", dArticleBottom)
                console.log("dContentsBottom", dContentsBottom)
                if (dContentsBottom >= dArticleBottom) {
                    // Make d-contents invisible
                    dContents.style.visibility = 'hidden';
                } else {
                    // Make d-contents visible
                    dContents.style.visibility = 'visible';
                }

                // Adjust the condition to account for margin-top
                if (scrollPosition + marginTop >= originalOffsetTop) {
                    dContents.style.position = 'fixed';
                    dContents.style.top = '0px';
                    dContents.style.left = originalOffsetLeft + 'px'; // Maintain the original horizontal position
                    dContents.style.width = originalWidth + 'px'; // Maintain the original width
                } else {
                    dContents.style.position = '';
                    dContents.style.top = '';
                    dContents.style.left = '';
                    dContents.style.width = ''; // Allow the width to be automatic
                }

                
            });

            
        
            // Initialize width and position
            onResize();
        </script>
        
        <script>
            // Function to determine which section is in view
            function getActiveSection() {
                var sections = document.querySelectorAll('section'); // Assuming your sections have a 'section' tag
                var scrollPosition = window.pageYOffset || document.documentElement.scrollTop;
        
                for (var i = 0; i < sections.length; i++) {
                    if (sections[i].offsetTop <= scrollPosition && sections[i].offsetTop + sections[i].offsetHeight > scrollPosition) {
                        return sections[i].id;
                    }
                }
                return null;
            }
        
            // Function to update the navigation items
            function updateNavigation() {
                var activeSection = getActiveSection();
                var navLinks = document.querySelectorAll('d-contents nav a');
        
                navLinks.forEach(function(navLink) {
                    if (navLink.getAttribute('href') === '#' + activeSection) {
                        navLink.classList.add('active-nav-item');
                    } else {
                        navLink.classList.remove('active-nav-item');
                    }
                });
            }
        
            // Add the scroll event listener
            window.addEventListener('scroll', updateNavigation);
        
            // Initial update
            updateNavigation();

                    // Insert citations and update bibliography on page load
            
        </script>
        
        
        

    </body>
</html>






<!--  -->

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Image Category Viewer</title>
    <style>
        body {
            font-family: Charter, serif;
            margin: 0;
            padding: 0px;
            box-sizing: border-box;
         }


    </style>
</head>
<body>
