<!doctype html>
<html lang="en">
    <head>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <script src="template.v2.js"></script>
        <script src="contents_bar.js"></script>
        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>
        <script src="cross_fade.js"></script>
        <link rel="stylesheet" href="style.css">
        
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
        

            
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>MMVP</title>

            
            <style>
                /* Styling for the table */
                table, th, td {
                    border: 1px solid black;
                    border-collapse: collapse;
                }
                th, td {
                    padding: 10px;
                    text-align: left;
                    cursor: pointer;
                }
                .container {
                    display: flex;
                    align-items: start;
                }

                .table-container {
                    margin-right: 10px;
                }


                /* Styling for the carousel */
                .carousel-container {
                    width: 100%;
                    max-width: 600px;
                    margin: auto;
                    position: relative;
                    overflow: hidden;
                    align-items: center;
                }
                .carousel-slide {
                    display: none;
                    width: 100%;
                    height: 400px;
                    position: center;
                    display: flex;
                    justify-content: center;
                    align-items: center;
                }
                .carousel-slide.active {
                    display: flex;
                }
                .carousel-slide img {
                    max-width: 100%;
                    max-height: 400px;
                    width: auto;
                    height: auto;
                    margin: 0%;
                    transform: translateX(150px);
                }
                .carousel-controls {
                    position: absolute;
                    top: 50%;
                    width: 100%;
                    display: flex;
                    justify-content: space-between;
                    transform: translateY(-50%);
                }
                .carousel-button {
                    background: none;
                    border: none;
                    cursor: pointer;
                    font-size: 24px;
                }
                .carousel-caption p {
                    text-align: center;
                    font-size: 16px;
                    color: #333;
                    margin-top: 10px; /* Adjust as needed */
                }
        
                /* Additional styles can go here */
            </style>

    </head>
    <body>
        <div class="header-container">
            <div class="header-content">
              <h1>Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs</h1>
              <div class="button-container">
                <a href="https://arxiv.org/abs/2401.06209" class="button">Paper</a>
                <a href="https://github.com/tsb0601/MMVP" class="button">Code</a>
              </div>
            </div>
            <div class="header-image">
                <img src="images/teaser_head.png" alt="Teaser Image" class="teaser-image">
            </div>
        </div>
    <d-article>
        <div class="byline">
            <div class="byline-container">
                <div class="byline-column">
                    <h3>Authors</h3>
                    <p><a href="https://tsb0601.github.io/petertongsb/" class="author-link">Shengbang Tong</a></p>
                    <p><a href="https://liuzhuang13.github.io/" class="author-link">Zhuang Liu</a></p>
                    <p><a href="https://yx-s-z.github.io/" class="author-link">Yuexiang Zhai</a></p>
                    <p><a href="https://people.eecs.berkeley.edu/~yima/" class="author-link">Yi Ma</a></p>
                    <p><a href="https://yann.lecun.com/" class="author-link">Yann LeCun</a></p>
                    <p><a href="https://www.sainingxie.com/" class="author-link">Saining Xie</a></p>
                </div>
                <div class="byline-column">
                    <h3>Affiliations</h3>
                    <p>New York University</a></p>
                    <p>Meta AI</a></p>
                    <p>UC Berkeley</a></p>
                    <p>UC Berkeley</a></p>
                    <p>New York University</a></p>
                    <p>New York University</a></p>
                </div>
                <div class="byline-column">
                    <h3>Date</h3>
                    <p>Jan. 08, 2024</p>
                </div>
            </div>
        </div>
        

        
        <div class="carousel-container">
            <div class="carousel-slide active">
                <img src="images/failure1.png" alt="Teaser Image 1">
            </div>
            <div class="carousel-slide active">
                <img src="images/failure2.png" alt="Teaser Image 2">
            </div>
            <div class="carousel-slide active">
                <img src="images/failure3.png" alt="Teaser Image 3">
            </div>
            <div class="carousel-slide active">
                <img src="images/failure4.png" alt="Teaser Image 4">
            </div>
            <div class="carousel-slide active">
                <img src="images/failure5.png" alt="Teaser Image 5">
            </div>
            <div class="carousel-slide active">
                <img src="images/failure6.png" alt="Teaser Image 6">
            </div>

            <!-- ... more slides ... -->

            <div class="carousel-controls">
                <button class="carousel-button" onclick="moveSlide(-1)">&#10094;</button>
                <button class="carousel-button" onclick="moveSlide(1)">&#10095;</button>
            </div>
            <div class="carousel-caption">
                <p>Failure Instances of GPT-4V, Swipe to see more</p>
            </div>
        </div>

        
        

    <script>
    let slideIndex = 0;
    showSlides(slideIndex);


    var bibliography = {
        "1": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021",
        "2": "Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023",
        "3": "Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng HuatTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards generalpurpose vision-language models with instruction tuning, 2023",
        "4": "Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. MiniGPT-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023",
        "5": "OpenAI. GPT-4V(ision) System Card, 2023",
        "6": "Google. Gemini, 2023",
        "7": "Google. Bard, 2023",
        "8": "Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks. arXiv preprint arXiv:2309.17425, 2023",
        "9": "Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying CLIP data. arXiv preprint arXiv:2309.16671, 2023",
        "10": "Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. EVA-CLIP: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023",
        "11": "Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. DINOv2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023.",
    };

    // Function to insert citation
    function insertCitation(refId) {
        document.getElementById("ref" + refId).innerText = "[" + refId + "]";
    }

    // Function to update bibliography section
    function updateBibliography() {
        var bibliographySection = document.getElementById('bibliography');
        for (var key in bibliography) {
            if (bibliography.hasOwnProperty(key)) {
                var para = document.createElement("p");
                para.innerHTML = "[" + key + "] " + bibliography[key];
                bibliographySection.appendChild(para);
            }
        }
    }

    // Insert citations and update bibliography on page load
    document.addEventListener('DOMContentLoaded', (event) => {
        for (var i = 1; i <= 12; i++) {
            insertCitation(i.toString());
        }
        updateBibliography();
    });
    


    function cite(key) {
            var citationText = bibliography[key];
            if (citationText) {
                return "[" + key + "]";
            }
            return "[Citation not found]";
        }

    function moveSlide(n) {
        showSlides(slideIndex += n);
    }

    function showSlides(n) {
        let i;
        let slides = document.getElementsByClassName("carousel-slide");
        if (n >= slides.length) { slideIndex = 0 }
        if (n < 0) { slideIndex = slides.length - 1 }
        for (i = 0; i < slides.length; i++) {
            slides[i].style.display = "none";
        }
        slides[slideIndex].style.display = "block";
    }
    // This ensures the script runs after the HTML document is fully parsed
    document.addEventListener('DOMContentLoaded', (event) => {
            updateBibliography();
        });

</script>
        
        <d-contents>
            <nav>
                <h4>Contents</h4>
                <div><a href="#mmvp">Searching for Visual Mistakes in MLLMs</a></div>
                <div><a href="#patterns">Visual Patterns that challenges CLIP Models</a></div>
                <div><a href="#mof">Mixture-of-Features improves MLLMs</a></div>
            </nav>
        </d-contents>
        
        <p>Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on
            the instance-level contrastive language-image pre-training (CLIP) <span id="ref1"></span>. Our research reveals that the visual capabilities in recent MultiModal LLMs (MLLMs) still exhibit systematic shortcomings. 
        </p>
        <section id="introduction">
            <h2>Searching for Visual Mistakes in CLIP and MLLM models</h2>
            <p>
               To understand the visual incapilities of multimodal LLMs, we delve into the visual encoder (CLIP models). We find ambiguities in CLIP embedding via "clip-blind pairs": Images that are visually different yet encoded similarly by CLIP models.  
               <d-figure>
                <figure>
                    <img src="images/pipeline.png" alt="MMVP Framework">
                    <figcaption>We start with finding CLIP-blind pairs that have similar CLIP embedding but different DINOv2 embedding. We manually inspect the differences between pair-wise images and formulate
                        questions based on the differences in the images. We then ask MLLMs the question alongside the CLIP-blind pair. The model receives a score only when both questions for the CLIP-blind pair are answered correctly.
                    </figcaption></figcaption>
                </figure>
            </d-figure>
            </p>
            <p>
                We assess the questions on SOTA open-source models (LLaVA-1.5 <span id="ref2"></span>, InstructBLIP <span id="ref3"></span>, Mini-GPT4 <span id="ref4"></span>) and closed-source models (GPT-4V <span id="ref5"></span>, Gemini <span id="ref6"></span>,
                Bard <span id="ref7"></span>). We also evaluate huamn performance through user studies. There is a significant performance gap between human and MLLM models, despite the latter often demonstrating impressive results. Models except GPT-4V and Gemini, scored below random guess level
                (25%). Most advanced GPT-4V and Gemini also face challenges in addressing basic visual grounding questions. 
                <figure>
                    <img src="images/result.png" alt="MMVP Framework">
                    <figcaption> There is a huge gap between human performance and MLLM's performance on the simple visual questions in MMVP Benchmark. 
                    </figcaption></figcaption>
                </figure>
            </p>
        </section>
        <section id="visualpatterns">
            <h2>Visual Patterns that challenge CLIP models</h2>

          
            
           
            <p>
                Having identified the CLIP-blind pairs, we summarize systematic visual patterns that the CLIP vision encoders might consistently misinterpret. We turn to the questions and options from the MMVP benchmark. With these questions, we transform abstract visual patterns in images into clearer, language-based descriptors that are easier to categorize. We identify 9 visual patterns:  
                

                <div class="container">
                    <!-- Table Container -->
                    <div class="table-container">
                        <table>
                            <tr onclick="showImage('orientation')">
                                <th><i class="fa fa-compass"></i></th>
                                <td>Orientation and Direction</td>
                            </tr>
                            <tr onclick="showImage('presence')">
                                <th><i class="fa fa-search"></i></th>
                                <td>Presence of Specific Features</td>
                            </tr>
                            <tr onclick="showImage('state')">
                                <th><i class="fa fa-sync"></i></th>
                                <td>State and Condition</td>
                            </tr>
                            <tr onclick="showImage('quantity')">
                                <th><i class="fa fa-sort-numeric-up"></i></th>
                                <td>Quantity and Count</td>
                            </tr>
                            <tr onclick="showImage('color')">
                                <th><i class="fa fa-palette"></i></th>
                                <td>Color and Appearance</td>
                            </tr>
                            <tr onclick="showImage('positional')">
                                <th><i class="fa fa-map-pin"></i></th>
                                <td>Positional and Relational Context</td>
                            </tr>
                            <tr onclick="showImage('structural')">
                                <th><i class="fa fa-cogs"></i></th>
                                <td>Structural and Physical Characteristics</td>
                            </tr>
                            <tr onclick="showImage('texts')">
                                <th><i class="fa fa-font"></i></th>
                                <td>Text</td>
                            </tr>
                            <tr onclick="showImage('viewpoint')">
                                <th><i class="fa fa-camera"></i></th>
                                <td>Viewpoint and Perspective</td>
                            </tr>
                            <!-- Repeat for other categories -->
                        </table>
                    </div>
            
                    <!-- Image Placeholder -->
                    <div>
                        <img id="displayedImage" src="images/Failure Categories/orientation.png"  width="100%" alt="Category Image">
                    </div>
                </div>
            
                <script>
                    var lastCategory = 'orientation';
            
                    function showImage(category) {
                        var imagePath = "images/Failure Categories/" + category + ".png";
                        var displayedImage = document.getElementById('displayedImage');
            
                        if(lastCategory === category) {
                            displayedImage.style.display = displayedImage.style.display === 'none' ? 'block' : 'none';
                        } else {
                            displayedImage.src = imagePath;
                            displayedImage.style.display = 'block';
                            lastCategory = category;
                        }
                    }
                </script>

                (Click on Visual Patterns to see examples)





               
            
                            </p>
            <h3 id="Scale">Scaling Up CLIP Doesn't Help Visual Patterns</h3>
            <p>
                CLIP models develop and scale over the years. We evaluate MMVP on a variety of CLIP models <span id="ref8"></span><span id="ref9"></span><span id="ref10"></span>. These models vary in aspects like size, training data, and methodology. As evidenced in the table, increasing network size and training data only aids in identifying two visual patterns – “color and appearance”
                and “state and condition”. The rest of the visual patterns continue to challenge all CLIP-based models
                <d-figure>
                    <img src="images/clip.png" alt="SEAL-Bench Results" width="100%" style="display: block; margin-left: auto; margin-right: auto;">
                    <figcaption> Models scaled up in resolution show minimal improvement, whereas a slight advantage is observed when scaling up the network. </figcaption></d-figure>

            </p>

            <h3 id="Correlation">CLIP mistakes and MLLMs Mistakes are Correlated</h3>
            <p>

                We plot CLIP’s performance and MLLMs' performance for each visual pattern. When the CLIP vision encoder underperforms on a certain visual pattern, the MLLM tends to exhibit similar shortcomings. Open-source models such as LLaVA 1.5 and InstructBLIP that explicitly use the CLIP vision encoder display a strong correlation in performance.
                <d-figure>
                    <img src="images/correlation.png" alt="SEAL-Bench Results" width="100%" style="display: block; margin-left: auto; margin-right: auto;">
                    <figcaption> If CLIP performs poorly on a visual pattern such as ``orientation'', MLLMs also underperform on the visual pattern. </figcaption></d-figure>

            </p>
        </section>
        
        <section id="MoF">
            <h2>Mixture-Of-Features (MoF) MLLM</h2>  
            <p>
                <em>If open-sourced MLLM's visual shortcomings come from the CLIP vision encoder, how do we build a more competent visual encoder?</em> We take initial steps to answer the question by studying Mixture-of-Features (MoF) that mixs Vision-Only SSL (DINOv2 <span id="ref11"></span>) features and CLIP features. 
                          <d-figure>
                        <img src="images/mof_pipeline.png" alt="SEAL-Bench Results" width="100%" style="display: block; margin-left: auto; margin-right: auto;">
                        <figcaption>Different Mixture-of-Feature (MoF) Strategies in MLLM. Left: Standard MLLM that uses CLIP as off-the-shelf pretrained vision encoder; Middle: Additive-MoF (A-MoF) MLLM: Linearly mixing CLIP and DINOv2 features before the adapter; Right: InterleavedMoF (I-MoF MLLM) Spatially interleaving CLIP visual tokens and DINOv2 visual tokens after the adapter.
                        </figcaption>
                </d-figure>
            </p>

            <h3 id="A-MoF">Vision-Only SSL features: Better Vision, Worse Language</h3>
            <p>

                We add a pretrained DINOv2 encoder into MLLM and linearly mix the CLIP pretrained encoder with it. Our study reveals that 
                <d-figure>
                    <img src="images/A-MoF.png" alt="SEAL-Bench Results" width="50%" style="display: block; margin-left: auto; margin-right: auto;">

                <ol>
                    <li>As the proportion of DINOv2 features increases, MLLM exhibits a decline in its instruction-following capability. Notably, there is a sharp decrease when the DINOv2 proportion reaches 87.5%.</li>
                    <li>A higher proportion of DINOv2 features enhances the model’s visual grounding capability, but this advantage diminishes when the DINOv2 proportion surpasses 0.75, at which point instruction-following is notably impaired.</li>
                </ol>
            </p>

            <h3 id="I-MoF">Interleaved-MoF: Combining advantages from CLIP and DINOv2 features</h3>
            <p>
                We propose interleaved MoF to leverage advantages from both CLIP and DINOv2 embeddings to enhance image representation. We take the processed features from CLIP and DINOv2 and interleave them while maintaining their original spatial order.  Interleave MoF significantly enhances visual grounding, with a 10.7% increase observed in MMVP, without compromising the model’s ability to follow instructions. This experiment is replicated with the LLaVA-1.5 setting and under various image resolution settings, yielding similar enhancements in performance.
                <d-figure>
                    <img src="images/I-MoF.png" alt="SEAL-Bench Results" width="65%" style="display: block; margin-left: auto; margin-right: auto;">
                    <figcaption> Interleaved MoF improves visual grounding while maintaining same level of instruction following ability. </figcaption></d-figure>

            </p>
        </section>
        
        </d-article>
        <d-appendix>
            <h3>BibTeX</h3>
            <p class="bibtex">
                @misc{tong2024eyes,<br>
                &nbsp;&nbsp;title={Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs},<br>
                &nbsp;&nbsp;author={Shengbang Tong and Zhuang Liu and Yuexiang Zhai and Yi Ma and Yann LeCun and Saining Xie}<br>
                &nbsp;&nbsp;year={2024},<br>
                &nbsp;&nbsp;eprint={2401.06209},<br>
                &nbsp;&nbsp;archivePrefix={arXiv},<br>
                &nbsp;&nbsp;primaryClass={cs.CV}<br>
                }
            </p>

            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
          
        
        <div id="bibliography">
            <h3>Bibliography</h3>
            <ul>
                   
                </ul>
            </div>
        </d-appendix>

     
        <!-- <script type="text/bibliography">
        
        </script> -->
        <script>
          
            var dContents = document.querySelector('d-contents');
            var dArticle = document.querySelector('d-article');
            // Get the computed style of the element to access the margin
            var computedStyle = window.getComputedStyle(dContents);
            // Get the top margin as an integer
            var marginTop = parseInt(computedStyle.marginTop, 10);
            // Calculate the original top offset plus the margin-top
            var originalOffsetTop = dContents.offsetTop;
            var originalOffsetLeft = dContents.offsetLeft;
            var originalWidth = dContents.offsetWidth; // This should include padding if box-sizing is border-box
        
            // Function to handle the resize event
            function onResize() {
                // Recalculate original left and width on resize
                originalOffsetLeft = dContents.offsetLeft;
                originalWidth = dContents.offsetWidth; // This should include padding if box-sizing is border-box
            }
        
            // Add the resize event listener
            window.addEventListener('resize', onResize);
        
            window.addEventListener('scroll', function() {
                var scrollPosition = window.pageYOffset || document.documentElement.scrollTop;
                var dArticleBottom = dArticle.offsetTop + dArticle.offsetHeight;
                var dContentsActualTop = scrollPosition > originalOffsetTop ? scrollPosition : originalOffsetTop;
                var dContentsBottom = dContentsActualTop + dContents.offsetHeight;
                console.log("dArticleBottom", dArticleBottom)
                console.log("dContentsBottom", dContentsBottom)
                if (dContentsBottom >= dArticleBottom) {
                    // Make d-contents invisible
                    dContents.style.visibility = 'hidden';
                } else {
                    // Make d-contents visible
                    dContents.style.visibility = 'visible';
                }

                // Adjust the condition to account for margin-top
                if (scrollPosition + marginTop >= originalOffsetTop) {
                    dContents.style.position = 'fixed';
                    dContents.style.top = '0px';
                    dContents.style.left = originalOffsetLeft + 'px'; // Maintain the original horizontal position
                    dContents.style.width = originalWidth + 'px'; // Maintain the original width
                } else {
                    dContents.style.position = '';
                    dContents.style.top = '';
                    dContents.style.left = '';
                    dContents.style.width = ''; // Allow the width to be automatic
                }

                
            });

            
        
            // Initialize width and position
            onResize();
        </script>
        
        <script>
            // Function to determine which section is in view
            function getActiveSection() {
                var sections = document.querySelectorAll('section'); // Assuming your sections have a 'section' tag
                var scrollPosition = window.pageYOffset || document.documentElement.scrollTop;
        
                for (var i = 0; i < sections.length; i++) {
                    if (sections[i].offsetTop <= scrollPosition && sections[i].offsetTop + sections[i].offsetHeight > scrollPosition) {
                        return sections[i].id;
                    }
                }
                return null;
            }
        
            // Function to update the navigation items
            function updateNavigation() {
                var activeSection = getActiveSection();
                var navLinks = document.querySelectorAll('d-contents nav a');
        
                navLinks.forEach(function(navLink) {
                    if (navLink.getAttribute('href') === '#' + activeSection) {
                        navLink.classList.add('active-nav-item');
                    } else {
                        navLink.classList.remove('active-nav-item');
                    }
                });
            }
        
            // Add the scroll event listener
            window.addEventListener('scroll', updateNavigation);
        
            // Initial update
            updateNavigation();

                    // Insert citations and update bibliography on page load
            
        </script>
        
        
        

    </body>
</html>
