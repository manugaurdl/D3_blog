<!doctype html>
<html lang="en">
    <head>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <script src="template.v2.js"></script>
        <script src="contents_bar.js"></script>
        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>
        <script src="cross_fade.js"></script>
        <link rel="stylesheet" href="style.css">
        
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
        

            
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>Detect-Describe-Discriminate</title>

            
            <style>
                ol {
                    margin-top: 0;
                    margin-bottom: 0;
                    list-style-position: inside; /* Adjust if necessary */

                }

                li {
                    line-height: 0.8;
                    padding-left: 100px; /* Adjust the padding value as needed */
                }
                /* Styling for the table */

                .pod-button-container {
                    display: flex;
                    justify-content: space-between;
                    margin-bottom: 10px; /* Vertical margin, adjust as needed */
                }
                .category-button {
                    padding: 10px 20px;
                    background-color: transparent; /* Default state */
                    border: 1px solid #ccc;
                    border-radius: 5px;
                    cursor: pointer;
                    font-size: 16px;
                    color: #000;
                    margin-right: 10px;
                    transition: background-color 0.3s, color 0.3s; /* Smooth transition */
                }

                .category-button:active, .category-button:focus {
                    background-color: rgb(149, 44, 15); /* Change background to blue when clicked or focused */
                    color: white; /* Change text to white */
                }
                .image-container {
                    border: 0px solid #ccc;
                    padding: 0px;
                    text-align: center;
                    margin-left:-20px;
                    margin-top:20px;
                    margin-bottom:20px;
                }
                #displayedImage {
                    max-width: auto;
                    height: 300px;
                }
                table, th, td {
                    border: 1px solid black;
                    border-collapse: collapse;
                }
                th, td {
                    padding: 10px;
                    text-align: left;
                    cursor: pointer;
                }
                .container {
                    display: flex;
                    align-items: start;
                    /* margin: 0 auto; */
                }

                .table-container {
                    margin-right: 10px;
                }

                /* Styling for the carousel */
                .carousel-container {
                    width: auto; 
                    margin-bottom: 2em;
                    position: relative;
                    overflow: hidden;
                    display: flex;
                    flex-direction: column; /* Stack elements vertically */
                    align-items: center;
                }

                .carousel-slide {
                    display: none;
                    width: auto;
                    height: auto;
                    position: relative;
                    display: flex;
                    justify-content: center;
                    align-items: center;
                    flex-direction: column; /* Stack image and caption vertically */
                }

                .carousel-slide.active {
                    display: flex;
                }

                .carousel-slide img {
                    width: 600px; /* Adjustable width */
                    height: auto; 
                    object-fit: contain;
                    margin: 0;
                }

                /* Caption Styling */
                .carousel-caption {
                    text-align: center;
                    margin-top: 10px;
                }

                .carousel-caption p {
                    font-size: 16px;
                    color: #333;
                    margin: 0;
                }

                .carousel-controls {
                    position: absolute;
                    top: 50%;
                    width: 100%;
                    display: flex;
                    justify-content: space-between;
                    transform: translateY(-50%);
                }

                .carousel-button {
                    background: none;
                    border: none;
                    cursor: pointer;
                    font-size: 24px;
                }
                
                .header-container {
                    background-color: transparent; /* Dark teal background */
                    color: black;
                    text-align: center; /* Center content on small screens */
                    padding: 20px; /* Reduced padding for small screens */
                }

                /* Additional styles can go here */
            </style>

    </head>
    <body>
        <div class="header-container">
            <div class="header-content">
              <h1>Detect, Describe, Discriminate: Moving Beyond VQA for MLLM Evaluation </h1>
              <div class="button-container">
                <a href="https://arxiv.org/" class="button">Paper</a>
                <a href="https://github.com/manugaurdl/detect-describe-discriminate/" class="button">Code</a>
              </div>
            </div>
            <div class="header-image">
                <img src="images/robot_flux.jpg" alt="Teaser Image" class="teaser-image">
                <figcaption>
                    This image was generated using Flux.1 [dev].
                </figcaption>
            </div>
        </div>
    <d-article>



        <div class="byline">
            <div class="byline-container">
                <p>
                    <a href="https://manugaurdl.github.io/" class="author-link" target="_blank" style="font-size: 1.3em; font-family: Charter;">Manu Gaur</a> &emsp;
                    <a href="https://darshansingh11.github.io/" class="author-link" target="_blank" style="font-size: 1.3em; font-family: Charter;">Darshan Singh S</a> &emsp;
                    <a href="https://makarandtapaswi.github.io/" class="author-link" target="_blank" style="font-size: 1.3em; font-family: Charter;">Makarand Tapaswi</a> &emsp;
                    <p></p>
                    <a href="https://www.iiit.ac.in/" class="affiliation-link" id="affiliation" target="_blank" style="font-size: 1.4em; font-family: Charter;">CVIT, IIIT Hyderabad</a>


                </p>
            </div>
        </div>
        
        <div class="carousel-container">
            <div class="carousel-slide active">
                <img src="images/vqa_5.jpg" alt="Teaser Image 1">
            </div>
            <div class="carousel-slide active">
                <img src="images/vqa_6.jpg" alt="Teaser Image 1">
            </div>
            <div class="carousel-slide active">
                <img src="images/vqa_2.jpg" alt="Teaser Image 2">
            </div>
            <div class="carousel-slide active">
                <img src="images/vqa_3.jpg" alt="Teaser Image 3">
            </div>
            <div class="carousel-slide active">
                <img src="images/vqa_4.jpg" alt="Teaser Image 4">
            </div>

            <div class="carousel-controls">
                <button class="carousel-button" onclick="moveSlide(-1)">&#10094;</button>
                <button class="carousel-button" onclick="moveSlide(1)">&#10095;</button>
            </div>
            <div class="carousel-caption">
                <p><i>Current MLLMs fail during D3 evaluation despite achieving perfect score in VQA, Swipe for more</i></p>
            </div>
        </div>

        
        

    <script>
    let slideIndex = 0;
    showSlides(slideIndex);


    var bibliography = {
        "1": "Manu Gaur, Darshan Singh S, Makarand Tapaswi. No Detail Left Behind: Revisiting Self-Retrieval for Fine-Grained Image Captioning. arXiv preprint arXiv:2409.03025, 2024",
        "2": "Xihui Liu, Hongsheng  Li, Jing  Shao, Dapeng Chen and Xiaogang Wang. Show, Tell and Discriminate: Image captioning by self-retrieval with partially labeled data. ECCV, 2024",
        "3": "Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs. arXiv preprint arXiv:2406.16860, 2024",
        "4": "Chameleon Team. Chameleon: Mixed-modal Early-Fusion Foundation Models. arXiv preprint arXiv:2405.09818, 2024.",
        "5": "Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-Next-Interleave: Tackling Multi-Image, Video, and 3D in Large Multimodal Models. arXiv preprint arXiv:2407.07895, 2024",
        "6": "Google. Gemini, 2023",
        "7": "OpenAI. GPT-4o, 2024",
        "8": "Anthropic. Claude-Sonnet, 2024",
        "9": "Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, YannLeCun, and Saining Xie. Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs. In Conference on Computer Vision and Pattern Recognition (CVPR), 2024",
        "10": "Penghao Wu and Saining Xie. V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs. In Conference on Computer Vision and Pattern Recognition (CVPR), 2024.",
    };

    // Function to insert citation
    function insertCitation(refId) {
        document.getElementById("ref" + refId).innerText = "[" + refId + "]";
    }

    // Function to update bibliography section
    function updateBibliography() {
        var bibliographySection = document.getElementById('bibliography');
        for (var key in bibliography) {
            if (bibliography.hasOwnProperty(key)) {
                var para = document.createElement("p");
                para.innerHTML = "[" + key + "] " + bibliography[key];
                bibliographySection.appendChild(para);
            }
        }
    }

    // Insert citations and update bibliography on page load
    document.addEventListener('DOMContentLoaded', (event) => {
        for (var i = 1; i <= 12; i++) {
            insertCitation(i.toString());
        }
        updateBibliography();
    });
    


    function cite(key) {
            var citationText = bibliography[key];
            if (citationText) {
                return "[" + key + "]";
            }
            return "[Citation not found]";
        }

    function moveSlide(n) {
        showSlides(slideIndex += n);
    }

    function showSlides(n) {
        let i;
        let slides = document.getElementsByClassName("carousel-slide");
        if (n >= slides.length) { slideIndex = 0 }
        if (n < 0) { slideIndex = slides.length - 1 }
        for (i = 0; i < slides.length; i++) {
            slides[i].style.display = "none";
        }
        slides[slideIndex].style.display = "block";
    }
    // This ensures the script runs after the HTML document is fully parsed
    document.addEventListener('DOMContentLoaded', (event) => {
            updateBibliography();
        });

</script>
        
        <d-contents>
            <nav>
                <h4>Contents</h4>
                <div><a href="#mmvp">Moving Beyond VQA</a></div>
                <div><a href="#patterns">MLLMs struggle with fine-grained Visual Discrimination</a></div>
                <div><a href="#patterns">Points of Difference in D<sub>3</sub> image pairs</a></div>
                <div><a href="#mof">Whitebox Evaluation of MLLMs</a></div>
            </nav>
        </d-contents>
        
        <p>
            Given an image pair, it is easier for an MLLM to discern fine-grained differences when prompted with a question and/or multiple choices (VQA evaluation) than to independently detect and describe such differences (our evaluation). Our work finds that state-of-the-art MLLMs struggle to discern fine-grained difference with our Detect-Describe-Discriminate (D<sub>3</sub>) evaluation framework, with open-source MLLMs failing to outperform random guess. 

        </p>
        <section id="introduction">
            <h2>Moving Beyond VQA</h2>
        </section>
        <p>
            VQA with multiple choices provides a reliable method for checking the existence of specific visual abilities. However, we find that it is <i>easier for the model to select an answer from multiple choices than to generate the answer itself </i>. Specifically, providing the answer along with task prompt (through multiple choice options or as part of the question) biases the MLLM's output towards the visual concept that is being evaluated.
        </p>
        <h3 id="Scale">Self-Retrieval Evaluation</h3>
        <p>
            In this work, we offer a novel perspective for fine-grained evaluation: we assess how well an MLLM understands a specific visual concept by its ability to uniquely describe two extremely similar images that differ only in the targeted visual concept.
            Unlike VQA, we do not constrain the output to multiple choice answers and directly evaluate the model's free-form generation.
            Specifically, we assess the MLLMs ability to capture subtle visual distinctions using self-retrieval evaluation , i.e., <i>by retrieving the target image using its generated caption against the other image in the pair serving as the distractor</i> <span id="ref1"></span> <span id="ref2"></span>.
        </p>
        <section id="D3">
            <h2>MLLMs struggle with fine-grained Visual Discrimination</h2>
            <p>
                We curate extremely similar image pairs, each having one prominent point of visual difference such that uniquely describing each image within the pair entails capturing a specific facet of visual understanding. With 247 such image pairs, we introduce the <strong>D<sub>3</sub></strong> benchmark. For each image in the pair, we prompt the model to:
                <ol>
                    <li><strong>Detect </strong> the visual difference,</li>
                    <li><strong>Describe</strong> the target image uniquely such that it,</li>
                    <li><strong>Discriminates </strong> the target image from the distractor.</li>
                </ol>

            </p>
            <p>
                <br>
                We benchmark state-of-the-art open-source (Cambrian-34B <span id="ref3"></span>, Chameleon-30b <span id="ref4"></span>, LLaVA-NeXT-34B <span id="ref5"></span>) and closed-source(GPT-4o <span id="ref7"></span>, Gemini-1.5-Pro <span id="ref6"></span>, Claude-Sonnet-3.5 <span id="ref8"></span>) MLLMs on the D<sub>3</sub> benchmark with self-retrieval evaluation. Results reveal that current MLLMs struggle to incorporate fine-grained visual details in their captions. Open-source models such as Cambrian-34B and LLaVA-NeXT-34B fail to even outperform random guess (25%). 
                Although, closed-source models outscore random guess, they still struggle to discern fine-grained visual differences, with Claude-Sonnet-3.5 achieving the highest score of 45.7% on our benchmark.

                <figure>
                    <img src="images/baselines.jpg" alt="MMVP Framework">
                    <figcaption> Both closed and open-source MLLMs stuggle to uniquely describe image from pairs in D<sub>3</sub> benchmark. 
                    </figcaption></figcaption>
                </figure>
            </p>
        </section>
        <section id="pods">
            <h2>Points of Difference in D<sub>3</sub> image pairs</h2>
           
            <p>
                Each image pair in D<sub>3</sub> are visually identical with one prominent <i>Point of Difference (POD)</i> distinguishing them. Similar to MMVP <span id="ref9"></span>, we identify 6 fine-grained <i>POD</i> that MLLMs struggle to identify: 

                <div class="container">
                    <div class="pod-button-container">
                        <button class="category-button" onclick="showImage('state')">State</button>
                        <button class="category-button" onclick="showImage('camera')">Camera</button>
                        <button class="category-button" onclick="showImage('orientation')">Orientation/Direction</button>
                        <button class="category-button" onclick="showImage('positioning')">Positioning</button>
                        <button class="category-button" onclick="showImage('scene')">Scene</button>
                        <button class="category-button" onclick="showImage('clutter')">Clutter</button>
                    </div>
                </div>
                
                <div class="container">
                    <div class="image-container">
                        <img id="displayedImage" src="images/POD/state.jpg" alt="Category Image">
                    </div>
                </div>
                <script>
                    var lastCategory = '';
                    function showImage(category) {
                        var imagePath = "images/POD/" + category + ".jpg";
                        var displayedImage = document.getElementById('displayedImage');
                        if (lastCategory === category) {
                            displayedImage.style.display = displayedImage.style.display === 'none' ? 'block' : 'none';
                        } else {
                            displayedImage.src = imagePath;
                            displayedImage.style.display = 'block';
                            lastCategory = category;
                        }
                    }
                </script>

                (Click on a specifc POD to see example)
               
            
            </p>
        </section>
        
        <section id="MoF">
            <h2>Whitebox Evaluation of MLLMs </h2>  
            <p>
                Since image pairs in D<sub>3</sub> benchmark have only one prominent POD, Self-Retrieval within D<sub>3</sub> enables whitebox evaluation. Specifically, when an MLLM fails to uniquely describe and retrieve both images within a pair, we are able to accurately identify which one of the following visual concept the MLLM is unable to pick up: 
        
                <d-figure>
                <img src="images/whitebox_eval.jpg" alt="SEAL-Bench Results" width="100%" style="display: block; margin-left: auto; margin-right: auto;">
                <figcaption>
                <strong>Whitebox Evaluation with D<sub>3</sub>:</strong> Self-retrieval performance of various MLLMs on image pairs of D<sub>3</sub> across all six points of difference.
                </figcaption>
                </d-figure>
            </p>
                We find that state-of-the-art MLLMs, both open and closed, struggle to perceive fine-grained changes in orientation/direction, camera angle, object's state, or positioning. This corroborates the findings of MMVP. In contrast, uniquely describing similar images with differing scenes appears to be easier for these models. V<sup>*</sup> <span id="ref10"></span> finds that MLLMs struggle to focus on fine-grained details in visually crowded images. While this is true for Cambrian-34B, closed-source MLLMs are capable of identifying characteristics of non-prominent objects to distinguish cluttered images.
            <p>

            </p>
        </section>
        
        </d-article>
        <d-appendix>
            <h3>BibTeX</h3>
            <p class="bibtex">
                @misc{gaur2024no,<br>
                &nbsp;&nbsp;title={Detect, Describe, Discriminate: Moving Beyond VQA for MLLM Evaluation},<br>
                &nbsp;&nbsp;author={Manu Gaur and Darshan Singh S and Makarand Tapaswi}<br>
                &nbsp;&nbsp;year={2024},<br>
                &nbsp;&nbsp;eprint={},<br>
                &nbsp;&nbsp;archivePrefix={arXiv},<br>
                &nbsp;&nbsp;primaryClass={cs.CV}<br>
                }
            </p>

            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
          
        
        <div id="bibliography">
            <h3>Bibliography</h3>
            <ul>
                   
                </ul>
            </div>  
        </d-appendix>

     
        <script>
          
            var dContents = document.querySelector('d-contents');
            var dArticle = document.querySelector('d-article');
            // Get the computed style of the element to access the margin
            var computedStyle = window.getComputedStyle(dContents);
            // Get the top margin as an integer
            var marginTop = parseInt(computedStyle.marginTop, 10);
            // Calculate the original top offset plus the margin-top
            var originalOffsetTop = dContents.offsetTop;
            var originalOffsetLeft = dContents.offsetLeft;
            var originalWidth = dContents.offsetWidth; // This should include padding if box-sizing is border-box
        
            // Function to handle the resize event
            function onResize() {
                // Recalculate original left and width on resize
                originalOffsetLeft = dContents.offsetLeft;
                originalWidth = dContents.offsetWidth; // This should include padding if box-sizing is border-box
            }
        
            // Add the resize event listener
            window.addEventListener('resize', onResize);
        
            window.addEventListener('scroll', function() {
                var scrollPosition = window.pageYOffset || document.documentElement.scrollTop;
                var dArticleBottom = dArticle.offsetTop + dArticle.offsetHeight;
                var dContentsActualTop = scrollPosition > originalOffsetTop ? scrollPosition : originalOffsetTop;
                var dContentsBottom = dContentsActualTop + dContents.offsetHeight;
                console.log("dArticleBottom", dArticleBottom)
                console.log("dContentsBottom", dContentsBottom)
                if (dContentsBottom >= dArticleBottom) {
                    // Make d-contents invisible
                    dContents.style.visibility = 'hidden';
                } else {
                    // Make d-contents visible
                    dContents.style.visibility = 'visible';
                }

                // Adjust the condition to account for margin-top
                if (scrollPosition + marginTop >= originalOffsetTop) {
                    dContents.style.position = 'fixed';
                    dContents.style.top = '0px';
                    dContents.style.left = originalOffsetLeft + 'px'; // Maintain the original horizontal position
                    dContents.style.width = originalWidth + 'px'; // Maintain the original width
                } else {
                    dContents.style.position = '';
                    dContents.style.top = '';
                    dContents.style.left = '';
                    dContents.style.width = ''; // Allow the width to be automatic
                }

                
            });

            
        
            // Initialize width and position
            onResize();
        </script>
        
        <script>
            // Function to determine which section is in view
            function getActiveSection() {
                var sections = document.querySelectorAll('section'); // Assuming your sections have a 'section' tag
                var scrollPosition = window.pageYOffset || document.documentElement.scrollTop;
        
                for (var i = 0; i < sections.length; i++) {
                    if (sections[i].offsetTop <= scrollPosition && sections[i].offsetTop + sections[i].offsetHeight > scrollPosition) {
                        return sections[i].id;
                    }
                }
                return null;
            }
        
            // Function to update the navigation items
            function updateNavigation() {
                var activeSection = getActiveSection();
                var navLinks = document.querySelectorAll('d-contents nav a');
        
                navLinks.forEach(function(navLink) {
                    if (navLink.getAttribute('href') === '#' + activeSection) {
                        navLink.classList.add('active-nav-item');
                    } else {
                        navLink.classList.remove('active-nav-item');
                    }
                });
            }
        
            // Add the scroll event listener
            window.addEventListener('scroll', updateNavigation);
        
            // Initial update
            updateNavigation();

                    // Insert citations and update bibliography on page load
            
        </script>
        
        
        

    </body>
</html>






<!--  -->

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Image Category Viewer</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0px;
            box-sizing: border-box;
        }


    </style>
</head>
<body>

