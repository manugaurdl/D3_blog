<!doctype html>
<html lang="en">
    <head>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <script src="./static/js/template.v2.js"></script>
        <script src="./static/js/contents_bar.js"></script>
        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>
        <script src="./static/js/cross_fade.js"></script>
        <link rel="stylesheet" href="style.css">
        
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
        

            
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>No-Detail-Left-Behind</title>

            
            <style>
                .contents-sec {
                    color: black;
                    text-decoration: none;
                }

                /* Active class for clicked links */
                .contents-sec.active {
                    color: blue;
                }
                html {
                scroll-behavior: smooth;
                }                

                .bibtex {
                font-family: 'Courier New', Courier, monospace;
                background-color: #f9f9f9;
                padding: 2em;
                color: #222;
                font-size: 1em;
                width:770px
                }

                d-article {
                overflow: visible; /* Ensure the article container allows overflow */
                }
                p {
                font-size: 20px; /* Adjust the pixel value as needed */
                }
                .figcaption-font {
                font-size: 17px; /* Adjust the font size as needed */
                }

                ol {
                    margin-top: 0;
                    margin-bottom: 0;
                    list-style-position: inside; /* Adjust if necessary */

                }

                li {
                    line-height: 0.8;
                    padding-left: 100px; /* Adjust the padding value as needed */
                }
                /* Styling for the table */

                .pod-button-container {
                    display: flex;
                    justify-content: space-between;
                    margin-bottom: 10px; /* Vertical margin, adjust as needed */
                }
                .category-button {
                    padding: 10px 20px;
                    background-color: transparent; /* Default state */
                    border: 1px solid #ccc;
                    border-radius: 5px;
                    cursor: pointer;
                    font-size: 16px;
                    color: #000;
                    margin-right: 10px;
                    transition: background-color 0.3s, color 0.3s; /* Smooth transition */
                }

                .category-button:active, .category-button:focus {
                    background-color: rgb(149, 44, 15); /* Change background to blue when clicked or focused */
                    color: white; /* Change text to white */
                }
                .image-container {
                    border: 0px solid #ccc;
                    padding: 0px;
                    text-align: center;
                    margin-left:-20px;
                    margin-top:20px;
                    margin-bottom:20px;
                }
                #displayedImage {
                    max-width: auto;
                    height: 300px;
                }
                table, th, td {
                    border: 1px solid black;
                    border-collapse: collapse;
                }
                th, td {
                    padding: 10px;
                    text-align: left;
                    cursor: pointer;
                }
                .container {
                    display: flex;
                    align-items: start;
                    /* margin: 0 auto; */
                }

                .table-container {
                    margin-right: 10px;
                }

                /* Styling for the carousel */
                .carousel-container {
                    width: auto; 
                    margin-bottom: 2em;
                    position: relative;
                    overflow: hidden;
                    display: flex;
                    flex-direction: column; /* Stack elements vertically */
                    align-items: center;
                }

                .carousel-slide {
                    display: none;
                    width: auto;
                    height: auto;
                    position: relative;
                    display: flex;
                    justify-content: center;
                    align-items: center;
                    flex-direction: column; /* Stack image and caption vertically */
                }

                .carousel-slide.active {
                    display: flex;
                }

                .carousel-slide img {
                    width: 600px; /* Adjustable width */
                    height: auto; 
                    object-fit: contain;
                    margin: 0;
                }

                /* Caption Styling */
                .carousel-caption {
                    text-align: center;
                    margin-top: 10px;
                }

                .carousel-caption p {
                    font-size: 16px;
                    color: #333;
                    margin: 0;
                }

                .carousel-controls {
                    position: absolute;
                    top: 50%;
                    width: 100%;
                    display: flex;
                    justify-content: space-between;
                    transform: translateY(-50%);
                }

                .carousel-button {
                    background: none;
                    border: none;
                    cursor: pointer;
                    font-size: 24px;
                }
                
                .header-container {
                    background-color: rgb(255, 249, 200);
                    color: rgb(242, 149, 83);
                    text-align: center; /* Center content on small screens */
                    padding: 20px; /* Reduced padding for small screens */
                }

                /* Additional styles can go here */
            </style>

    <!-- Zoom -->
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js"></script>  <!-- jquery -->
    <script defer src="./static/js/medium-zoom.min.js"></script>
    <script defer src="./static/js/zoom.js"></script>
    </head>
    <body>
        <div class="header-container">
            <div class="header-content">
              <h1 style="margin-left: 80px;"><i>No Detail Left Behind</i>:<br> Revisiting Self-Retrieval for Fine-Grained Image Captioning</h1>
              <div class="button-container" style="margin-left: 80px;">
                <a href="https://arxiv.org/abs/2409.03025" class="button">Paper</a>
                <a href="https://github.com/manugaurdl/no-detail-left-behind" class="button">Code</a>
              </div>
            </div>
            <div class="header-image" style="width:800px;margin-top: 10px;">
                <img src="images/header_teaser.png" alt="Teaser Image" class="teaser-image">
                <figcaption>
                    This image was generated using Flux.1 [schnell].
                </figcaption>
            </div>
        </div>
    <d-article>



        <div class="byline">
            <div class="byline-container">
                <p>
                    <a href="https://manugaurdl.github.io/" class="author-link" target="_blank" style="font-size: 1.3em; font-family: Charter;">Manu Gaur</a> &emsp;
                    <a href="https://darshansingh11.github.io/" class="author-link" target="_blank" style="font-size: 1.3em; font-family: Charter;">Darshan Singh S</a> &emsp;
                    <a href="https://makarandtapaswi.github.io/" class="author-link" target="_blank" style="font-size: 1.3em; font-family: Charter;">Makarand Tapaswi</a> &emsp;
                    <p></p>
                    <a href="https://www.iiit.ac.in/" class="affiliation-link" id="affiliation" target="_blank" style="font-size: 1.4em; font-family: Charter;">CVIT, IIIT Hyderabad</a>


                </p>
            </div>
        </div>

    
    <script>
    let slideIndex = 0;
    showSlides(slideIndex);


    var bibliography = {
        "1": "Manu Gaur, Darshan Singh S, Makarand Tapaswi. No Detail Left Behind: Revisiting Self-Retrieval for Fine-Grained Image Captioning. arXiv preprint arXiv:2409.03025, 2024",
        "2": "Xihui Liu, Hongsheng  Li, Jing  Shao, Dapeng Chen and Xiaogang Wang. Show, Tell and Discriminate: Image captioning by self-retrieval with partially labeled data. ECCV, 2024",
        "3": "Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs. arXiv preprint arXiv:2406.16860, 2024",
        "4": "Chameleon Team. Chameleon: Mixed-modal Early-Fusion Foundation Models. arXiv preprint arXiv:2405.09818, 2024.",
        "5": "Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-Next-Interleave: Tackling Multi-Image, Video, and 3D in Large Multimodal Models. arXiv preprint arXiv:2407.07895, 2024",
        "6": "Google. Gemini, 2023",
        "7": "OpenAI. GPT-4o, 2024",
        "8": "Anthropic. Claude-Sonnet, 2024",
        "9": "Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, YannLeCun, and Saining Xie. Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs. In Conference on Computer Vision and Pattern Recognition (CVPR), 2024",
        "10": "Penghao Wu and Saining Xie. V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs. In Conference on Computer Vision and Pattern Recognition (CVPR), 2024.",
    };

    // Function to insert citation
    function insertCitation(refId) {
        document.getElementById("ref" + refId).innerText = "[" + refId + "]";
    }

    // Function to update bibliography section
    function updateBibliography() {
        var bibliographySection = document.getElementById('bibliography');
        for (var key in bibliography) {
            if (bibliography.hasOwnProperty(key)) {
                var para = document.createElement("p");
                para.innerHTML = "[" + key + "] " + bibliography[key];
                bibliographySection.appendChild(para);
            }
        }
    }

    // Insert citations and update bibliography on page load
    document.addEventListener('DOMContentLoaded', (event) => {
        for (var i = 1; i <= 12; i++) {
            insertCitation(i.toString());
        }
        updateBibliography();
    });
    


    function cite(key) {
            var citationText = bibliography[key];
            if (citationText) {
                return "[" + key + "]";
            }
            return "[Citation not found]";
        }

    function moveSlide(n) {
        showSlides(slideIndex += n);
    }

    function showSlides(n) {
        let i;
        let slides = document.getElementsByClassName("carousel-slide");
        if (n >= slides.length) { slideIndex = 0 }
        if (n < 0) { slideIndex = slides.length - 1 }
        for (i = 0; i < slides.length; i++) {
            slides[i].style.display = "none";
        }
        slides[slideIndex].style.display = "block";
    }
    // This ensures the script runs after the HTML document is fully parsed
    document.addEventListener('DOMContentLoaded', (event) => {
            updateBibliography();
        });

    //make d-contents a href blue
    document.addEventListener('DOMContentLoaded', function() {
            const links = document.querySelectorAll('.contents-sec');
            
            // Add event listener for click on each .contents-sec link
            links.forEach(link => {
                link.addEventListener('click', function(e) {
                    // Remove 'active' class from all .contents-sec links
                    links.forEach(l => l.classList.remove('active'));
                    
                    // Add 'active' class to the clicked link
                    this.classList.add('active');
                });
            });
        });
</script>
        


        <!------------------------------------------------INTRODUCTION ---------------------------------------------------------------->
        <d-contents>
            <nav>
                <p class="contents-sec" style="text-transform: uppercase; font-size: 17px; font-family:Charter; line-height:1.4em; font-weight:bold"">contents</p>
                <div><a href="#intro" style="font-size: 16px; font-family:Charter; line-height:1.4em; font-weight:bold">TL;DR</a></div>
                <div><a href="#truematch" style="font-size: 16px; font-family:Charter; line-height:1.4em; font-weight:bold">Evaluation</a></div>
                <div><a href="#vcb" style="font-size: 16px; font-family:Charter; line-height:1.4em; font-weight:bold">Data</div>
                <div><a href="#training" style="font-size: 16px; font-family:Charter; line-height:1.4em; font-weight:bold">Training</a></div>
            </nav>
        </d-contents>


        <section id="intro">
        <p style="font-size: 22px;">
            Image captioning systems are unable to generate fine-grained captions, often using generic descriptions that fail to discriminate visually similar images. 
            We use <a href="#sr_setup">self-retrieval</a> to <i> improve <strong>training</strong> and <strong>evaluation</strong> of captioning systems.</i></p>
        <d-figure id="sr_setup">
            <figure>
                <img data-zoomable="" draggable="false" src="images/sr_setup.png" style="width: 600px; height: auto; display: block; margin: 0 auto;margin-top: 50px;">
                <br>
                <figcaption class="figcaption-font" style="text-align: center; width: 600px; margin: 0 auto;">
                    <strong>Figure 1</strong>: Self-retrieval judges the ability of a captioner to retrieve an image using its <i>generated caption</i> against of bag of distractor images. <a href="https://arxiv.org/pdf/2304.01662">Image source</a>.
                        
                </figcaption>
            </figure>
        </d-figure>
        <p style="font-size: 22px;margin-top: 10px;">

            Our work is built upon three key components, each focused on making captioning systems more fine-grained:            
        </p>


            <ol style="margin-top: 40px">
            </ol>


            <ol start="1">
                <li style="line-height: 1.5; font-size: 20px;"><strong><a href="#truematch"> Evaluation</a></strong>: Traditional image caption evaluation is neither fine-grained nor rewards diversity.  Hence, we present <strong><i>TrueMatch</i></strong>, a benchmark that uses self-retrieval to assess the captioner's ability to capture subtle visual distinctions.</li>
                <li style="line-height: 1.5; font-size: 20px;"><strong><a href="#vcb">Data</a></strong>: 
                    Training on noisy (alt-text) or generic (human annotations) datasets bottlenecks fine-grained caption generation. To this extent, we present <strong><i>Visual Caption Boosting</i></strong> to instill fine-grainedness in generic image captioning datasets while remaining anchored in human annotations.</li>
                <li style="line-height: 1.5; font-size: 20px;"><strong><a href="#training">Self-Retrieval Training</a></strong>:
                    MLE training encourages generation of statistically probable phrases. 
                    We design a <strong><i>training recipe</i></strong> to address this
                    by instilling discriminant visual information in the captioner.   
            </ol>

            <p style="font-size: 22px;margin-top: 50px;">
            We find that captioning systems, irrespective of their size, struggle to capture fine-grained visual details leading to poor performance on <i>TrueMatch</i>. However, using our "plug and play" training recipe, we are able to outpefrom state-of-the-art open-source MLLMs while having 1-2 orders of magnitude fewer parameters.
            </p>
        </p>
        </section>
        <hr>
    




        <!---------------------------------------------Section 1: Evaluation----------------------------------------------------->
        <section id="truematch">
            <h2>I. <i>TrueMatch</i>: Fine-grained Evaluation of Captioning Systems</h2>
            <p style="font-size: 19px;">


                <d-figure id="teaser" >
                    <figure>
                        <img data-zoomable="" draggable="false" src="images/motorcycle_teaser.png" alt="motorcycles" style="width: 1000px; height: auto; display: block; margin: 0 auto;">
                        <br>
                        <figcaption class="figcaption-font" style="text-align: center; width: 1000px; margin: 0 auto;">
                            <strong>Figure 2</strong>:
                            <!-- For a set of similar images, captioning systems struggle to uniquely describe each image.  -->
                            <strong>An example from <i>TrueMatch</i>.</strong>
                            (<strong>COCO MLE</strong>) produces the same caption.
                            Vanilla self-retrieval training (<strong>COCO SR</strong>) hallucinates details such as “two people” (middle).
                            Our proposed method (<strong>OURS SR</strong>) generates discriminant captions.
                        </figcaption>
                    </figure>
                </d-figure>
                
                <p style="font-size: 19px;">TrueMatch comprises <strong>bags of highly similar images</strong> with <u>varying sizes</u>, where uniquely describing each image within a bag requires capturing various facets of visual understanding.
                It offers a comprehensive framework for using self-retrieval to holistically evaluate captioning systems.
                For example, retrieving the target image from <a href="#teaser"> Figure 2</a> requires the caption to incorporate information about attributes <i>(red bike)</i> or orientation <i>(inverted body)</i>.</p>

                
                <br>
            </p>
            <div style="display: flex; flex-wrap: wrap; gap: 20px; align-items: start;">
                <div style="flex: 1; min-width: 300px;margin-top: 10px;">
                    <p style="font-size: 19px;">
                    <strong style="font-size: 25px">Benchmarking Captioning Systems on <i>TrueMatch</i></strong>
                    <br><br>
                    <a href="#table_1">Table 1</a> evaluates several open-source captioning approaches, MLLMs, and SR trained models on TrueMatch. 
                    <ul>
                        <li style="line-height: 1.2; font-size: 19px;"> Irrespective of their size, captioners struggle to capture fine-grained visual details leading to poor performance on TrueMatch.</li>
                        <li style="line-height: 1.2; font-size: 19px;">  Our SR training recipe outperforms vanilla SR (DiscriTune) by +14.4% to +19.5%, demonstrating its effectiveness.</li>
                        <li style="line-height: 1.2; font-size: 19px;">  <strong>Cambrian-1</strong> is the best-performing open-source MLLM. Our proposed approach significantly outperforms it despite being 30x smaller.</li>
                    </ul>
                    </p>
                </div>
                
                <figure id ='table_1' style="flex: 1; min-width: 300px; margin: 0;margin-top:0px">
                    <img data-zoomable="" draggable="false" src="images/table_1.png" alt="table1" style="width: 550px; height: auto; display: block;">
                    <figcaption class="figcaption-font" style="text-align: center; margin-top: 10px; width:550px">
                        <strong>Table 1:</strong> Recall@1 for self-retrieval evaluation with TrueMatch is reported. The number
                        of bags in #3 is 254, #5 is 104, and #7 is 93
                    </figcaption>
                </figure>
            </div>  


        </section>
        
        <!----------------------------------------------Section 2: DATA------------------------------------------------------------>

        <section id="vcb">
            <h2>II. <i>Visual Caption Boosting</i></h2>
        <p style="font-size: 19px;">
            Existing image captioning datasets like MSCOCO comprise generic annotations. 
            While recent works address this by synthetically expanding visual information within the captions, they are prone to inherit biases present in foundation models.
            <!-- Additionally, these methods produce verbose descriptions that often exceed the token capacity of VLMs (CLIP with 77 tokens) and makes them prone to hallucinations (Favero et al., 2024). -->
            Interestingly, although the individual COCO captions are sparse, we find that they describe complementary facets of the image (see <a href="#vcb_teaser">Figure 3</a>).
            <br><br>
            VCB is a novel two stage framework leverages <u>foundation models</u>
            <!-- and the diverse perspectives offered by human annotators  -->
            to generate rich descriptions while being <u>anchored in human data</u>:
        </p>
        
        <d-figure id="vcb_teaser" >
            <figure>
                <img data-zoomable="" draggable="false" src="images/vcb_teaser.png" alt="table1" style="width: 1100px; height: auto; display: block; margin: 0 auto;">
                <br>
                <figcaption class="figcaption-font" style="text-align: center; width: 1100px; margin: 0 auto;">
                    <strong>Figure 3: VCB Framework.</strong> Generic COCO captions are visually boosted using complementary details present in human annotations VCB ignores specific details from the visual caption (“river or lake”, “drinking”), as they conflict with annotations (“watering hole”, “grazing and standing”).
                </figcaption>
            </figure>
        </d-figure>
        <p style="font-size: 19px;">
            <strong style="font-size: 20px">Stage 1: BlendCap</strong> uses an LLM to create a blended caption that combines diverse perspectives 
            <!-- multiple facets of visual information that  -->
            offered by different human annotations.
            Notably, <i>we prompt the LLM to minimize redundant information resulting in short descriptions</i>.
                
            <br><br>

            <strong style="font-size: 20px">Stage 2: HolisticCap</strong> 
            expands BlendCap by using an LLM to instill fine-grained details from the Visual Caption (InstructBLIP).
            The LLM is prompted to prefer human-grounded BlendCap in case of conflicting visual information.
            <br><br>
            Anchoring semantic visual information in human annotations <u><i>reduces verbose tendencies of MLLMs</i></u>, producing rich and succinct captions.
        </p>
        
        <div style="display: flex; flex-wrap: wrap; gap: 50px; align-items: start;">
            <div style="flex: 1; min-width: 300px;margin-top: 40px;">
                <p style="font-size: 19px;">
                <strong style="font-size: 25px">Benchmarking Visually Boosted Captions on <i>TrueMatch</i></strong>
                <br>
                <ul>
                    <li style="line-height: 1.4; font-size: 19px;">BlendCap significantly outperforms individual COCO captions on RD100 and  ClipScore, confirming that human annotations capture complementary visual aspects of the same image.
                    </li>
                    <li style="line-height: 1.4; font-size: 19px;">HolisticCap (unlike BlendCap) yields substantantial gains over COCO across bags of TrueMatch. 
                    </li>
                </ul>
                </p>
                
            </div>              
            
            <figure id ='table_2' style="flex: 1; min-width: 300px; margin: 0;">
                <img data-zoomable="" draggable="false" src="images/table_2.png" alt="table1" style="width: 450px; height: auto; display: block;margin-left:10px;margin-top:40px">
                <figcaption class="figcaption-font" style="text-align: center; margin-top: 10px; margin-left:-20px;margin-right: -80px; width:500px">
                    <strong>Table 2:</strong> : R@1 scores for COCO and VCB captions evaluated on RD100 (100 random distractors) and TrueMatch.
                </figcaption>
            </figure>
        </div>
    

        <d-figure>
            <figure>
                <img data-zoomable="" draggable="false" src="images/vcb_finding.png" alt="table1" style="width: 1100px; height: auto; display: block;margin-left: -10px; margin-top: 50px;">
                <br>
            </figure>
        </d-figure>


        <!---------------------------------------------Section 3: Training ----------------------------------------------------------->
        </section>
        <section id="training">
            <h2 style="margin-top: 10px;">III. Guiding captioners away from their language modeling priors</h2>

            <!-- CLIPCAP -->
            <div style="display: flex; flex-wrap: wrap; gap: 20px; width: 1500px; align-items: start;">
                <div style="flex: 1; margin-top: 30px;">
                    <p style="font-weight: normal; font-size: 19px;">
            
                        We adopt ClipCap (Mokady et al., 2021), a lightweight (200M) simplification of the modern MLLMs.
                        Training the captioning system has two stages: 
                        <ol>
                            <li style="line-height: 1; font-size: 19px;font-weight: normal;">Maximum Likelihood Estimation (MLE) pretraining.</li>
                            <li style="line-height: 1.2; font-size: 19px;font-weight: normal;">REINFORCE fine-tuning by maximizing the self-retrieval (SR) reward.</li>
                        </ol>
        
                    </p>
                </div>              
                
                <figure id ='clipcap' style="flex: 1; min-width: 300px; margin: 0;">
                    <img data-zoomable="" draggable="false" src="images/clipcap.jpg" alt="table1" style="width: 300px; height: auto; display: block;margin-left:0px;margin-top:0px">
                    <figcaption class="figcaption-font" style="text-align: center; margin-top: 0px; margin-left:-20px; width:350px">
                        <strong>Figure 4:</strong> : ClipCap connects a CLIP visual encoder to a GPT-2 through a MLP adapter.
                    </figcaption>
                </figure>
            </div>  
        
        <br><br>
        <p style="font-weight: normal; font-size: 19px;">
        <!------------------------------------------------SR-L --------------------------------------------------------------->
        <strong style="font-size: 25px;">Fine-tuning the Language Model with Self-Retrieval (SR-L): A Deep Dive</strong>
        <br><br>
        <strong>MLE encourages generation of generic descriptions.</strong> Retrieval performance of MLE trained captioner reduces substantially for all datasets (rows 1-3) compared to their ground-truth captions (<a href="#table_2">see Table 2</a>).

        <br><br>
        
        <strong> Self-retrieval fine-tuning benefits from a rich MLE initialization.</strong>
        SR-L fine-tuning with <span style="background-color: rgb(0, 255, 0); color: black;">HolisticCap</span> significantly outperforms <span style="background-color: lightgray; color: black;">COCO</span> (Dessì et al., 2023) on TrueMatch.
        </p>

        <d-figure id="table3" >
            <figure>
                <img data-zoomable="" draggable="false" src="images/table_3.png" alt="table1" style="width:660px; height: auto; display: block; margin: 0 auto;">
                <br>
                <figcaption class="figcaption-font" style="text-align: center; width: 660px; margin: 0 auto;">
                    <strong>Table 3:</strong> Results across different training approaches and captions show the effectiveness of the rich initialization provided by HolisticCap and SR-L fine-tuning.
                </figcaption>
            </figure>
        </d-figure>
        <p style="font-weight: normal; font-size: 19px;">
        <strong> Self-retrieval unlocks latent semantic information when fine-tuning the LLM. </strong>
        
        The model trained on COCO, due to the MLE objective, generates sparse captions that resemble the independent annotations of COCO. 
        This leads to a large gap between <span style="background-color: lightgray; color: black;">COCO</span> and <span style="background-color:lightgreen; color: black;">BlendCap</span> on RD100 (rows 1, 2), despite both having similar semantic information.
        Interestingly, SR-L fine-tuning narrows this gap dramatically (rows 4, 5).
        </p>
        <d-figure>
            <figure>
                <img data-zoomable="" draggable="false" src="images/srl_finding2.png" alt="table1" style="width: 1150px; height: auto; display: block;margin-left:0px; margin-top: 30px;">
                <br>
            </figure>
        </d-figure>
        
        <!---------------------------------------- SR-V ------------------------------------------>
        <p style="font-size: 25px; font-weight: bold;"> Fine-tuning the Visual Encoder with Self-Retrieval (SR-V)</p>
        <p style="font-size:19px;font-weight: normal;">
        
        While fine-tuning CLIP with SR yields superior retrieval performance, it makes the captioner less faithful to the ground-truth captions.
        </p>
        <br><br>
        <!------------------------------------------------TRADEOFF --------------------------------------------------------------->
        <p style="font-size: 25px; font-weight: bold;"> Retrieval Performance vs Caption Faithfulness : The Trade-off plaguing current SR approaches</p>

        <div style="display: flex; flex-wrap: wrap; gap: 20px; align-items: start;">
            <div style="flex: 1; min-width: 300px;margin-top: 20px; margin-left:20px ;">
                <p style="font-size:19px;font-weight: normal;">
                
                    <ul>
                        <li style="line-height: 1.2; font-size: 19px; font-weight: normal; margin-bottom: 30px;"> We observe that captioning system's fine-tuned with vanilla SR-L (Dessì et al. 2023) become less faithful to the ground-truth captions upon extended training.</li>
                        <li style="line-height: 1.2; font-size: 19px; font-weight: normal; margin-bottom: 30px">As seen in <a href="#tradoff">Figure 5</a>, retrieval performance continually improves while CIDEr dips significantly.</li>
                        <li style="line-height: 1.2; font-size: 19px; font-weight: normal;"> We also find that captioners fine-tuned with SR, in a dash to enhance retrieval performance, have a tendency to <strong>hallucinate details</strong>.
                        </li>
                        <li style="line-height: 1.2; font-size: 19px; font-weight: normal;">We also find that SR-V fine-tuning worsens <i>SR's tendency to hallucinate</i> and <i>attribute binding</i> in captioners.  </li>
                    </ul>
                </p>        
            </div>              
            
            <figure id ='tradeoff' style="flex: 1; min-width: 300px; margin: 0;">
                <img data-zoomable="" draggable="false" src="images/tradeoff.png" alt="table1" style="width: 450px; height: auto; display: block;margin-left:60px;margin-top:10px">
                <figcaption class="figcaption-font" style="text-align: center; margin-top:0px; margin-left:80px; width:450px;font-weight: normal;">
                    <strong>Figure 5:</strong> R@1 continually increases while CIDEr degrades when fine-tuning ClipCap with SR-L on COCO.
                </figcaption>
            </figure>
        </div>  

        
        <!------------------------------------------------BagCurri --------------------------------------------------------------->
        <br><br>
        <p style="font-size: 25px;font-weight: bold;"> BagCurri:  Curriculum Training with Bags of Hard Negatives</p>
        <p style="font-size:19px;font-weight: normal;">
        
        For a stronger learning signal, we fine-tune with bags of highly similar images within a minibatch instead of retrieving against 99 random distractors (vanilla SR).
        We also propose a curriculum over bag sizes (<a href="#bagcurri">see Figure 6</a>) to <i>more optimally leverage the contrastive SR reward</i>.</p>

        <strong style="font-size: 20px;"">SR-V fine-tuning with BagCurri</strong> (row 7)
        <ul style="padding-left: 30px;">
            <li style="line-height: 1.2; font-size: 19px; font-weight: normal; margin-bottom: 5px;">Forces CLIP to learn fine-grained visual features leading to <i>superior retrieval performance</i> on TrueMatch. </li>
            <li style="line-height: 1.2; font-size: 19px; font-weight: normal; margin-bottom: 0px">Reduces caption faithfulness (<span style="background-color: rgb(255, 204, 204); color: black; font-size:17px">row 7</span>), leading to increased hallucinations and worse attribute binding.</li>
        </ul>

        <strong style="font-size: 20px;"">SR-L fine-tuning with BagCurri</strong> (row 6)
        <ul style="padding-left: 30px;">
            <li style="line-height: 1.2; font-size: 19px; font-weight: normal; margin-bottom: 5px;">BagCurri is too challenging for SR-L fine-tuning, unable to yield meaningful retrieval gains over vanilla SR-L. </li>
            <li style="line-height: 1.2; font-size: 19px; font-weight: normal; margin-bottom: 0px">Benefits from rich MLE initialization provided by <u>HolisticCap</u>  (<span style="background-color: rgb(0, 255, 0); color: black; font-size:17px">row 6</span>): <i>CIDEr score increases</i> compared to vanilla SR-L</li>.
        </ul>
        <br>
        <d-figure id="bagcurri" style="overflow-x: auto; width: 100%;" >
            <figure>
                <img data-zoomable="" draggable="false" src="images/bagcurri_tab_fig.png" alt="table1" style="width: 1100px; height: auto; display: block;margin-left:-20px;">
                <br>
                <figcaption class="figcaption-font" style="text-align: center; width: 1100px; margin: 0 auto;margin-left: -20px;">
                    <strong>Table 4 (Left)</strong>: Impact of fine-tuning different components with BagCurri as compared against vanilla SR (SR-L). 
                    <strong>Figure 6 (Right)</strong>: Our curriculum progressively increases bag sizes during training.
                </figcaption>
            </figure>
        </d-figure>

        <p style="font-size:19px; font-weight: normal;">
        <strong style="font-size: 20px;"">BEST OF BOTH WORLDS: SR-LV fine-tuning with BagCurri</strong>.
        <br>We initialize the captioner with HolisticCap and fine-tune both the language and visual components (SR-LV) with BagCurri.
        This results in the most visually fine-grained model while also improving CIDEr over the MLE trained captioner.
        Notably, we find that rich initialization provided by <u>HolisticCap</u> and <u>BagCurri</u> is <i>solely responsible for preserving caption faithfulness</i>, with CIDEr decreasing for COCO (<span style="background-color: rgb(255, 102, 102); color: black; font-size:17px">row 4</span>) or when bags are used without our curriculum.
        </p>
        <d-figure>
            <figure>
                <img data-zoomable="" draggable="false" src="images/final_finding.png" alt="table1" style="width: 1150px; height: auto; display: block;margin-left: 0px; margin-top: 40px;">
                <br>
            </figure>
        </d-figure>
        </p>
        <br>
        <!------------------------------------------------CIDEr + SR --------------------------------------------------------------->
        <p style="font-size: 25px;font-weight: bold;"> CIDEr Optimization meets SR Fine-Tuning</p>

        <div style="display: flex; flex-wrap: wrap; gap: 20px; align-items: start;">
            <div style="flex: 1; min-width: 300px;margin-top: 20px; margin-left:20px ;">
                <p style="font-size:19px;font-weight: normal;">
                
                    <ul>
                        <li style="line-height: 1.2; font-size: 18.5px; font-weight: normal; margin-bottom: 30px;">CIDEr opt. (<span style="background-color: lightgreen; color: black; font-size:17px">row 8</span>) outperforms the MLE trained model (<span style="background-color:lightgray; color: black; font-size:17px">row 6</span>) on TrueMatch, only when initialized with HolisticCap.</li>
                        <li style="line-height: 1.2; font-size: 18.5px; font-weight: normal;">Even CIDEr opt. (<span style="background-color: rgb(255, 102, 102); color: black;; font-size:17px">row 5</span>) is unable to preseve caption faithfulness for COCO, underscoring the <u>importance of initialization</u>    during SR fine-tuning. </li>
                        <li style="line-height: 1.2; font-size: 18.5px; font-weight: normal; margin-bottom: 30px">Joint optimization with HolisticCap (<span style="background-color: rgb(0, 255, 0); color: black;; font-size:17px">row 10</span>) results in the most discriminant model while <i>significantly improving CIDEr </i> over MLE pretraining (<span style="background-color:lightgray; color: black;">row 6</span>).</li>
                        </li>
                    </ul>
                </p>        
            </div>           
            
            <figure id ='table_2' style="flex: 1; min-width: 300px; margin: 0;">
                <img data-zoomable="" draggable="false" src="images/table_5.png" alt="table1" style="width: 550px; height: auto; display: block;margin-left:30px;margin-top:-60px">
                <figcaption class="figcaption-font" style="text-align: center; margin-top:5px; margin-left:40px; width:550px;font-weight: normal;">
                    <strong>Table 5:</strong> Impact of combining SR with CIDEr optimization. C: CIDEr,<br> SR: Self-Retrieval, and BC: SR with BagCurri.
                </figcaption>
            </figure>
        </div>          
        <!------------------------------------------------DIVERSITY --------------------------------------------------------------->
        
        <p style="font-size: 25px;font-weight: bold; margin-top: 60px;"> Genating Diverse Captions</p>
        
        <div style="display: flex; flex-wrap: wrap; gap: 20px; align-items: start;">
            <div style="flex: 1; min-width: 300px;margin-top: 20px; margin-left:20px ;">
                <p style="font-size:19px;font-weight: normal;">
                Using our <u>training recipe with Visual Caption Boosting</u> guides the captioner away from the <i>language modelling priors</i>, improving caption diversity over <span style="background-color: rgb(212,212,212); color: black">COCO MLE</span> training by <strong>304%</strong>.
            </p>        
        </div>           
        
        <figure id ='table_2' style="flex: 1; min-width: 300px; margin: 0;">
            <img data-zoomable="" draggable="false" src="images/diversity.png" alt="table1" style="width: 450px; height: auto; display: block;margin-left:90px;margin-top:-20px">
            <figcaption class="figcaption-font" style="text-align: center; margin-top:5px; margin-left:100px; width:450px;font-weight: normal;">
                <strong>Table 6:</strong> : Number of words with frequency >= 5 on the COCO test set.
            </figcaption>
        </figure>
        </div>      
    
    
    
    <!------------------------------------------------END --------------------------------------------------------------->
        </d-article>
        <d-appendix>
            <h3 style="font-weight:bold">BibTeX</h3>
            <p class="bibtex">
                @misc{ndlb2024,<br>
                &nbsp;&nbsp;title={{No Detail Left Behind: Revisiting Self-Retrieval for Fine-Grained Image Captioning},<br>
                &nbsp;&nbsp;author={Manu Gaur and Darshan Singh S and Makarand Tapaswi}<br>
                &nbsp;&nbsp;year={2024},<br>
                &nbsp;&nbsp;eprint={2409.03025},<br>
                &nbsp;&nbsp;archivePrefix={arXiv},<br>
                &nbsp;&nbsp;primaryClass={cs.CV}<br>
                }
            </p>

            <h3 style="font-weight:bold">Bibliography</h3>
            <ol>
                <li style="font-family: 'Times New Roman', Times, serif;
                font-size: 1em;
                line-height: 1.5;
                text-indent: -15px;
                width: 1000px;">
                    Cross-Domain Image Captioning with Discriminative Finetuning.<br>
                    Roberto Dessì, Michele Bevilacqua, Eleonora Gualdoni, Nathanael Carraz Rakotonirina, Francesca Franzon, Marco Baroni. 2023. CVPR
                </li>
                <li style="font-family: 'Times New Roman', Times, serif;
                font-size: 1em;
                line-height: 1.5;
                text-indent: -15px;
                width: 1000px;">
                    ClipCap: CLIP Prefix for Image Captioning.<br>
                    Ron Mokady, Amir Hertz, Amit H. Bermano. 2021. arXiv:2111.09734
                </li>
            </ol>

            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
          
        
        <!-- <div id="bibliography">
            <h3 style="font-weight: bold;">Bibliography</h3>
            <ul>
                
            </ul>
        </div>   -->
            
            <h3>Acknowledgements</h3>
            <p style="font-family: 'Times New Roman', Times, serif;
            font-size: 1em;">
            This project page is built upon <a href="https://tsb0601.github.io/mmvp_blog/">MMVP</a>.
            </p>
        </d-appendix>

     
        <script>
          
            var dContents = document.querySelector('d-contents');
            var dArticle = document.querySelector('d-article');
            // Get the computed style of the element to access the margin
            var computedStyle = window.getComputedStyle(dContents);
            // Get the top margin as an integer
            var marginTop = parseInt(computedStyle.marginTop, 10);
            // Calculate the original top offset plus the margin-top
            var originalOffsetTop = dContents.offsetTop;
            var originalOffsetLeft = dContents.offsetLeft;
            var originalWidth = dContents.offsetWidth; // This should include padding if box-sizing is border-box
        
            // Function to handle the resize event
            function onResize() {
                // Recalculate original left and width on resize
                originalOffsetLeft = dContents.offsetLeft;
                originalWidth = dContents.offsetWidth; // This should include padding if box-sizing is border-box
            }
        
            // Add the resize event listener
            window.addEventListener('resize', onResize);
        
            window.addEventListener('scroll', function() {
                var scrollPosition = window.pageYOffset || document.documentElement.scrollTop;
                var dArticleBottom = dArticle.offsetTop + dArticle.offsetHeight;
                var dContentsActualTop = scrollPosition > originalOffsetTop ? scrollPosition : originalOffsetTop;
                var dContentsBottom = dContentsActualTop + dContents.offsetHeight;
                console.log("dArticleBottom", dArticleBottom)
                console.log("dContentsBottom", dContentsBottom)
                if (dContentsBottom >= dArticleBottom) {
                    // Make d-contents invisible
                    dContents.style.visibility = 'hidden';
                } else {
                    // Make d-contents visible
                    dContents.style.visibility = 'visible';
                }

                // Adjust the condition to account for margin-top
                if (scrollPosition + marginTop >= originalOffsetTop) {
                    dContents.style.position = 'fixed';
                    dContents.style.top = '0px';
                    dContents.style.left = originalOffsetLeft + 'px'; // Maintain the original horizontal position
                    dContents.style.width = originalWidth + 'px'; // Maintain the original width
                } else {
                    dContents.style.position = '';
                    dContents.style.top = '';
                    dContents.style.left = '';
                    dContents.style.width = ''; // Allow the width to be automatic
                }

                
            });

            
        
            // Initialize width and position
            onResize();
        </script>
        
        <script>
            // Function to determine which section is in view
            function getActiveSection() {
                var sections = document.querySelectorAll('section'); // Assuming your sections have a 'section' tag
                var scrollPosition = window.pageYOffset || document.documentElement.scrollTop;
        
                for (var i = 0; i < sections.length; i++) {
                    if (sections[i].offsetTop <= scrollPosition && sections[i].offsetTop + sections[i].offsetHeight > scrollPosition) {
                        return sections[i].id;
                    }
                }
                return null;
            }
        
            // Function to update the navigation items
            function updateNavigation() {
                var activeSection = getActiveSection();
                var navLinks = document.querySelectorAll('d-contents nav a');
        
                navLinks.forEach(function(navLink) {
                    if (navLink.getAttribute('href') === '#' + activeSection) {
                        navLink.classList.add('active-nav-item');
                    } else {
                        navLink.classList.remove('active-nav-item');
                    }
                });
            }
        
            // Add the scroll event listener
            window.addEventListener('scroll', updateNavigation);
        
            // Initial update
            updateNavigation();

                    // Insert citations and update bibliography on page load
            
        </script>
        
        
        

    </body>
</html>






<!--  -->

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Image Category Viewer</title>
    <style>
        body {
            font-family: Charter, serif;
            margin: 0;
            padding: 0px;
            box-sizing: border-box;
         }
  

    </style>
</head>
<body>
